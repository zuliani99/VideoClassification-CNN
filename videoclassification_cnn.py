# -*- coding: utf-8 -*-
"""VideoClassification-CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11dN68GBUbLUqI5cg8gdARur9vFK5PHT8

# Set Up Datasets
"""

#!sudo -H pip install --upgrade youtube-dl
#!sudo -H pip install --upgrade pytube

import os
from tqdm import tqdm
from multiprocessing import Pool
import cv2
import youtube_dl
import shutil
import time

def process_video(url, skip_frames, directory, id, total_frames):
    cap = cv2.VideoCapture(url)
    x = 0
    count = 0
    while count < total_frames:
        ret, frame = cap.read()
        
        if not ret: 
          print('Error can not open the video')
          break
        
        filename = f'{directory}/{id}/shot{str(x)}.png'
        x += 1
        cv2.imwrite(filename.format(count), cv2.resize(frame, (256, 144), interpolation = cv2.INTER_AREA))
        count += skip_frames
    cap.release()

def take_shots_from_url(video_url, directory='train_shoths', percentage_of_frames=3):
  try:
    id = video_url.split('=')[1]
    if not os.path.exists(f'./{directory}/{id}'): os.makedirs(f'./{directory}/{id}')

    #print("")
    ydl_opts = {}
    ydl = youtube_dl.YoutubeDL(ydl_opts)
    info_dict = ydl.extract_info(video_url, download=False)
    formats = info_dict.get('formats', None)

    video_length = info_dict['duration'] * info_dict['fps']
    tot_stored_frames = (video_length * percentage_of_frames) // 100
    if tot_stored_frames > 10: tot_stored_frames = 10 # I limit the number of stored images to 10 
    skip_rate = video_length // tot_stored_frames

    resolution_id = ['160', '133', '134', '135', '136']
    format_id = {}
    for f in formats: format_id[f['format_id']] = f
    for res in resolution_id:
      if res in list(format_id.keys()):
        video = format_id[res]
        url = video.get('url', None)
        if(video.get('url', None) != video.get('manifest_url', None)):
          #print(video.get('url', None), '         ', video.get('manifest_url', None),'         ', video.get('fragment_base_url', None))
          print(f'Obtaining frames of {video_url}, length {info_dict["duration"]}, fps: {info_dict["fps"]}, resolution: {res}p, skip rate: {skip_rate}')
          process_video(url, skip_rate, directory, id, info_dict['duration'] * info_dict['fps'])
          break
      else:
        print(f'No {res}p resolution found, trying a higher one')
  except Exception as e:
    print(e)
    shutil.rmtree(f'./{directory}/{id}')


if __name__ == "__main__":
  # DOWNOAD ZIP FILES
  #os.system("wget --no-verbose https://github.com/gtoderici/sports-1m-dataset/archive/refs/heads/master.zip")

  # EXTRACT AND DELETE THEM
  #os.system("unzip -qq -o master.zip")
  #os.system("rm master.zip")

  TRAIN, TEST = {}, {}
  DATA = [TRAIN, TEST]
  LABELS = []
  path = './sports-1m-dataset-master/original'

  for idx, f in enumerate(os.listdir(path)):
    with open(path + '/' + f) as f_txt:
      lines = f_txt.readlines()
      for line in lines:
        splitted_line = line.split(' ')
        label_indices = splitted_line[1].rstrip('\n').split(',') 
        DATA[idx][splitted_line[0]] = list(map(int, label_indices))

  with open('./sports-1m-dataset-master/labels.txt') as f_labels:
    LABELS = f_labels.read().splitlines()

  train_url_list = list(DATA[0].keys())
  test_url_list = list(DATA[1].keys())

  #print(len(train_url_list), len(test_url_list))

  pool = Pool(os.cpu_count())

  start_time = time.time()
  #Parallel(n_jobs=-1)(delayed(take_shots_from_url)(url, 'train_shoths', 3) for url in tqdm(train_url_list))
  pool.map(take_shots_from_url, tqdm(train_url_list))
  pool.close() 
  print("--- %s seconds ---" % (time.time() - start_time))

  #start_time = time.time()
  #Parallel(n_jobs=-1)(delayed(take_shots_from_url)(url, 'test_shoths', 3) for url in tqdm(test_url_list))
  #print("--- %s seconds ---" % (time.time() - start_time))
