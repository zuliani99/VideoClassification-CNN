{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing *yt-dlp* to download the image frame from the YouTube videos and *pyarrow* "
      ],
      "metadata": {
        "id": "c1O6qJVSxsun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e-szf_YfTZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c8aabe-15f7-4515-d1e0-6cd1bbe11f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2023.3.4-py2.py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli\n",
            "  Downloading Brotli-1.0.9-cp39-cp39-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 KB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mutagen\n",
            "  Downloading mutagen-1.46.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 KB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex\n",
            "  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from yt-dlp) (2022.12.7)\n",
            "Collecting websockets\n",
            "  Downloading websockets-10.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.0.9 mutagen-1.46.0 pycryptodomex-3.17 websockets-10.4 yt-dlp-2023.3.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.9/dist-packages (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from pyarrow) (1.24.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp\n",
        "!pip install pyarrow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all the useful packages"
      ],
      "metadata": {
        "id": "mjyedpkDx_p0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tcf31PT9fYKS"
      },
      "outputs": [],
      "source": [
        "import yt_dlp\n",
        "import os\n",
        "#from tqdm import tqdm \n",
        "from tqdm.notebook import tqdm\n",
        "import cv2\n",
        "import shutil\n",
        "import time\n",
        "import random\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import multiprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7Xtp0A6uTiN",
        "outputId": "7cf57877-1718-4699-996a-9d0bd7abf5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Importing drive to download or update datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGoLZ4_ofeKZ"
      },
      "source": [
        "# Setting Up Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52DVZ517fan1"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "percentage_train_test = 10\n",
        "percentage_bag_shots = 1\n",
        "percentage_to_ignore = 10\n",
        "\n",
        "zip_f_name = f'VideoClassificationDataset_{percentage_train_test}_{percentage_bag_shots}_{percentage_to_ignore}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlxRTLRo_TjR"
      },
      "source": [
        "##Download frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fxUL8gX6eJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e60a8288-8f59-427e-9d5e-ed45d7172500"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-26 12:43:54 URL:https://codeload.github.com/gtoderici/sports-1m-dataset/zip/refs/heads/master [150629222] -> \"master.zip\" [1]\n",
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ]
        }
      ],
      "source": [
        "dataset_path = Path('/content/VideoClassificationDataset')\n",
        "\n",
        "# Download the .zip file\n",
        "!wget --no-verbose https://github.com/gtoderici/sports-1m-dataset/archive/refs/heads/master.zip\n",
        "\n",
        "# Extract it \n",
        "!unzip -qq -o '/content/master.zip' \n",
        "!rm '/content/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xi72iBkwfdSO"
      },
      "outputs": [],
      "source": [
        "DATA = {'train_partition.txt': {},\n",
        "        'test_partition.txt': {}}\n",
        "\n",
        "LABELS = []\n",
        "\n",
        "train_dict = {}\n",
        "test_dict = {}\n",
        "\n",
        "path = '/content/sports-1m-dataset-master/original'\n",
        "\n",
        "# Populate the DATA dictionary by reading the train and test files\n",
        "for f in os.listdir(path):\n",
        "    with open(path + '/' + f) as f_txt:\n",
        "        lines = f_txt.readlines()\n",
        "        for line in lines:\n",
        "            splitted_line = line.split(' ')\n",
        "            label_indices = splitted_line[1].rstrip('\\n').split(',') \n",
        "            DATA[f][splitted_line[0]] = list(map(int, label_indices))\n",
        "\n",
        "# Obtain the labels from the relative .txt file\n",
        "with open('/content/sports-1m-dataset-master/labels.txt') as f_labels:\n",
        "    LABELS = f_labels.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGQKz8h5fpQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65beccc6-1f6e-4fbd-adca-5d8176a0280f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Train Test length:  914491 218667\n",
            "Sampled 10 % of Train & Test datasets, updated length:  91449 21866\n"
          ]
        }
      ],
      "source": [
        "TRAIN = DATA['train_partition.txt']\n",
        "TEST = DATA['test_partition.txt']\n",
        "print('Original Train Test length: ', len(TRAIN), len(TEST))\n",
        "\n",
        "# Sample a subset of percentage_train_test\n",
        "TRAIN = dict(random.sample(list(TRAIN.items()), (len(TRAIN)*percentage_train_test)//100))\n",
        "TEST = dict(random.sample(list(TEST.items()), (len(TEST)*percentage_train_test)//100))\n",
        "\n",
        "print(f'Sampled {percentage_train_test} % of Train & Test datasets, updated length: ', len(TRAIN), len(TEST))\n",
        "\n",
        "if not os.path.exists(dataset_path): os.makedirs(dataset_path)\n",
        "if not os.path.exists(f'{dataset_path}/train'): os.makedirs(f'{dataset_path}/train')\n",
        "if not os.path.exists(f'{dataset_path}/test'): os.makedirs(f'{dataset_path}/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8vNk0MIf0S_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE: Extract frames from a given youtube video\n",
        "\n",
        "TAKES:\n",
        "  - capture: cv2.VideoCapture(url) variable, with url being the link to the video\n",
        "  - directory: describe the saving directory\n",
        "  - idx_bag: indicate the index of the actual bag of shots\n",
        "  - start_frame: indicates the starting frame\n",
        "  - end_frame: indicates the ending frame\n",
        "\n",
        "RETURNS:\n",
        "  True or False depending the presence of errors\n",
        "'''\n",
        "\n",
        "def extract_frames(capture, directory, idx_bag, start_frame, end_frame):\n",
        "    count = start_frame\n",
        "\n",
        "    # Set the next frame to download to 'count'\n",
        "    capture.set(cv2.CAP_PROP_POS_FRAMES, count)\n",
        "    os.makedirs(f'{directory}/bag_of_shots{str(idx_bag)}') # Create the relative directory\n",
        "\n",
        "    # Download the frame until we do not reach the end_frame\n",
        "    while count < end_frame:\n",
        "\n",
        "        ret, frame = capture.read() # Read the frame\n",
        "\n",
        "        if not ret: # In case there are errors, delete the aforementioned directory of bag of shots\n",
        "            shutil.rmtree(f'{directory}/bag_of_shots{str(idx_bag)}')\n",
        "            return False\n",
        "\n",
        "        # Save the readed frame in the directory of bag of shots resizing it to be 178x178\n",
        "        filename = f'{directory}/bag_of_shots{str(idx_bag)}/shot{str(count - start_frame)}.png'\n",
        "        cv2.imwrite(filename, cv2.resize(frame, (178, 178), interpolation = cv2.INTER_AREA))\n",
        "        count += 1\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IfkSZDVf1zT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE: Determine the amount of frame and bag of shots we want to downloads in each video, and perform the download of them\n",
        "\n",
        "TAKES:\n",
        "  - video_url: complete url link to a specific YouTubbe video\n",
        "  - labels_list: list of indices labels of the relative video\n",
        "  - idx_bag: indicate the index of the actual bag of shots\n",
        "  - percentage_of_bags: indicate the percentage of bags of shots that we want to download\n",
        "\n",
        "RETURNS: \n",
        "  ret_dictionary: A dictionary containing as keys the path to a specific bag of shots and as value the list of labels for the \n",
        "                  relative bag, the last element of this list represent the number of frames we have downloaded\n",
        "'''\n",
        "\n",
        "def video_to_frames(video_url, labels_list, directory, percentage_of_bags):\n",
        "    url_id = video_url.split('=')[1]\n",
        "    path_until_url_id = f'{dataset_path}/{directory}/{url_id}'\n",
        "    \n",
        "    ret_dictionary = {}\n",
        "\n",
        "    try:   \n",
        "\n",
        "        # Setting up the dictionary options for yd-dlp\n",
        "        ydl_opts = {\n",
        "            'ignoreerrors': True,\n",
        "            'quiet': True,\n",
        "            'nowarnings': True,\n",
        "            'ignorenoformatserror': True,\n",
        "            'verbose':False,\n",
        "            'cookies': '/content/all_cookies.txt',\n",
        "            #https://stackoverflow.com/questions/63329412/how-can-i-solve-this-youtube-dl-429\n",
        "        }\n",
        "        ydl = yt_dlp.YoutubeDL(ydl_opts)\n",
        "        info_dict = ydl.extract_info(video_url, download=False) # Extracting the video infromation\n",
        "\n",
        "        if(info_dict is not None and  info_dict['fps'] >= 20):\n",
        "            # I must have a least 20 frames per seconds since I take half of second bag of shots for every video\n",
        "\n",
        "            formats = info_dict.get('formats', None)\n",
        "\n",
        "            # Excluding the initial and final choosen percentage of each video to avoid noise\n",
        "            video_length = info_dict['duration'] * info_dict['fps']\n",
        "\n",
        "            shots = info_dict['fps'] // 2\n",
        "\n",
        "            to_ignore = (video_length * percentage_to_ignore) // 100\n",
        "            new_len = video_length - (to_ignore * 2)\n",
        "            tot_stored_bags = ((new_len // shots) * percentage_of_bags) // 100   # ((total_possbile_bags // shots) * percentage_of_bags) // 100\n",
        "\n",
        "            if tot_stored_bags == 0: tot_stored_bags = 1 # I take at leasta bag of shots\n",
        "\n",
        "            # Computing the skip rate between bags\n",
        "            skip_rate_between_bags = (new_len - (tot_stored_bags * shots)) // (tot_stored_bags-1) if tot_stored_bags > 1 else 0\n",
        "\n",
        "            chunks = [[to_ignore+(bag*(skip_rate_between_bags+shots)), to_ignore+(bag*(skip_rate_between_bags+shots))+shots] for bag in range(tot_stored_bags)]\n",
        "            # Sequence of [[start_frame, end_frame], [start_frame, end_frame], [start_frame, end_frame], ...]\n",
        "\n",
        "            format_id = {}\n",
        "            for f in formats: format_id[f['format_id']] = f\n",
        "\n",
        "            if '160' in list(format_id.keys()): # The downlaod is applyed only if the 144p format is available, 160 is the format_id for 144p\n",
        "                video = format_id['160']\n",
        "                url = video.get('url', None)\n",
        "                if(video.get('url', None) != video.get('manifest_url', None)): # We move forward only in case the url is different from the manifest_url\n",
        "\n",
        "                    if not os.path.exists(path_until_url_id): os.makedirs(path_until_url_id)\n",
        "                    \n",
        "                    capture = cv2.VideoCapture(url) # Initialize VideoCapture variable\n",
        "                    valid_chunks = 0\n",
        "\n",
        "                    for idx_bag, f in enumerate(chunks): # For each chunks\n",
        "\n",
        "                        # In case the download of the bag of shots succedeed\n",
        "                        if(extract_frames(capture, path_until_url_id, idx_bag, f[0], f[1])):\n",
        "                          \n",
        "                            l = np.zeros(len(LABELS), dtype=int) \n",
        "                            for label in labels_list: l[label] = 1 # np.array of labels indices\n",
        "                            l = np.append(l, [shots]) # Appending as last element the number of downloaded frames\n",
        "                            valid_chunks += 1\n",
        "\n",
        "                            ret_dictionary[f'{directory}/{url_id}/bag_of_shots{str(idx_bag)}'] = l.tolist() # Populate the new dictionary row\n",
        "\n",
        "                    # In case we do not have downloaded any chunks delete the directory and all its content\n",
        "                    if valid_chunks == 0: shutil.rmtree(path_until_url_id)\n",
        "\n",
        "                    capture.release()\n",
        "\n",
        "        return ret_dictionary\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # If an exception rised delete the directory with all its content and return ret_dictionary\n",
        "        if os.path.exists(path_until_url_id): shutil.rmtree(path_until_url_id)\n",
        "        return ret_dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8j8kijof3ix"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "count = 0\n",
        "\n",
        "# Perform a parallel download\n",
        "with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as pool:\n",
        "    with tqdm(total = len(TRAIN.items()), leave=False, desc='Downloading Train Dataset') as progress: # Setting up the progress bar\n",
        "        \n",
        "        futures = [] # Array of features\n",
        "\n",
        "        for url, labels_list in TRAIN.items():\n",
        "            future = pool.submit(video_to_frames, url, labels_list, 'train', percentage_bag_shots) # Assign the download\n",
        "            future.add_done_callback(lambda p: progress.update()) # Update the progrress bar\n",
        "            futures.append(future) # Append the feature to the features array\n",
        "\n",
        "        for future in futures:\n",
        "            if len(future.result()) > 0: # In case the result of the featue is not empty\n",
        "                train_dict.update(future.result()) # Append the result to the final dictionaty\n",
        "                count += 1\n",
        "\n",
        "\n",
        "print(f'--- Downloaded {count} videos frames in {(timedelta(seconds=(time.time() - start_time)))} (H:M:S:ms) ---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgZgTx5Vf72y"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "count = 0\n",
        "\n",
        "# Perform a parallel download\n",
        "with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as pool:\n",
        "    with tqdm(total = len(TEST.items()), leave=False, desc='Downloading Test Dataset') as progress: # Setting up the progress bar\n",
        "        \n",
        "        futures = [] # Array of features\n",
        "\n",
        "        for url, labels_list in TEST.items():\n",
        "            future = pool.submit(video_to_frames, url, labels_list, 'test', percentage_bag_shots) # Assign the download\n",
        "            future.add_done_callback(lambda p: progress.update()) # Update the progrress bar\n",
        "            futures.append(future) # Append the feature to the features array\n",
        "\n",
        "        for future in futures:\n",
        "            if len(future.result()) > 0: # In case the result of the featue is not empty\n",
        "                test_dict.update(future.result()) # Append the result to the final dictionaty\n",
        "                count += 1\n",
        "\n",
        "\n",
        "print(f'--- Downloaded {count} videos frames in {(timedelta(seconds=(time.time() - start_time)))} (H:M:S:ms) ---')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMEdQJ3O_lAw"
      },
      "source": [
        "## Save CSV and upload file Zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M65GudI_f9UT"
      },
      "outputs": [],
      "source": [
        "# Convert the dictionary to pandas DataFrame resetting the index\n",
        "train_df = pd.DataFrame.from_dict(train_dict, orient='index', dtype=int).reset_index(level=0)\n",
        "train_df = train_df.rename(columns={train_df.columns[-1]: 'shots'}) # Rename the last column as 'shots'\n",
        "train_df.columns = train_df.columns.astype(str) # Setting the column type as string\n",
        "train_df.to_parquet(f'{dataset_path}/train.parquet', index=True) # Generation the parquet file\n",
        "\n",
        "# Convert the dictionary to pandas DataFrame resetting the index\n",
        "test_df = pd.DataFrame.from_dict(test_dict, orient='index', dtype=int).reset_index(level=0)\n",
        "test_df = test_df.rename(columns={test_df.columns[-1]: 'shots'}) # Rename the last column as 'shots'\n",
        "test_df.columns = test_df.columns.astype(str) # Setting the column type as string\n",
        "test_df.to_parquet(f'{dataset_path}/test.parquet', index=True) # Generation the parquet file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTkJhhbBuY4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e158f2fc-b71d-43a0-9215-702383213600"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Datasets_VideoClassification/VideoClassificationDataset_1_1_10.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# Copy the labels.txt in the dataset fodler\n",
        "shutil.copy('/content/sports-1m-dataset-master/labels.txt', dataset_path)\n",
        "shutil.make_archive(zip_f_name, 'zip', dataset_path) # Create a zip file\n",
        "shutil.copy(f'/content/{zip_f_name}.zip','/content/drive/MyDrive/Datasets_VideoClassification') # Upload the zip file in the drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMTDALJo_tYE"
      },
      "source": [
        "## Load Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zipped Dataset"
      ],
      "metadata": {
        "id": "qiJSveSdw04r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr0R9hdz1Ih1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377,
          "referenced_widgets": [
            "422ae7830e3740caa81197397880bd39",
            "13e906e348e44ea19febd28714295e58",
            "f2af354e9ad24460aae635f52e98dc3d",
            "4770452fcc8d485c8ab8dae12c7937f9",
            "51e40810a55d4ff587b4ec5452a5776e",
            "cd1ab633a60f438e96549fb829066b30",
            "b1235c12bfcc4c3a9926e0216a533a19",
            "fb79c74d916d42799dd7ac341fa69b0a",
            "b271af095877404dac02f67142e56ca8",
            "d5c02c7993044fedb2c06a0637fe732a",
            "d59184f34bc4413baf07e2f3efe26514"
          ]
        },
        "outputId": "6f5accea-dbb2-42f6-bb3a-4af1f4cf218a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1946590 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "422ae7830e3740caa81197397880bd39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-45b8320b7977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/content/drive/MyDrive/Datasets_VideoClassification/{zip_f_name}.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(self, member, path, pwd)\u001b[0m\n\u001b[1;32m   1623\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1695\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mfdst_write\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsrc_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36m_read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    742\u001b[0m                         \"Close the writing handle before trying to read.\")\n\u001b[1;32m    743\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "dataset_path = Path('/content/VideoClassificationDataset')\n",
        "\n",
        "# Unzip the dataset zip file in Colab\n",
        "with zipfile.ZipFile(f'/content/drive/MyDrive/Datasets_VideoClassification/{zip_f_name}.zip', 'r') as zip_ref:\n",
        "    for file in tqdm(iterable=zip_ref.namelist(), total=len(zip_ref.namelist())):\n",
        "        zip_ref.extract(member=file, path=dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzipped Dataset"
      ],
      "metadata": {
        "id": "Jy1d8LVywqvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = Path('/content/drive/MyDrive/Datasets_VideoClassification/VideoClassificationDataset')\n",
        "\n",
        "#with zipfile.ZipFile(f'/content/drive/MyDrive/Datasets_VideoClassification/{zip_f_name}.zip', 'r') as zip_ref:\n",
        "    #for file in tqdm(iterable=zip_ref.namelist(), total=len(zip_ref.namelist())):\n",
        "        #zip_ref.extract(member=file, path=dataset_path)"
      ],
      "metadata": {
        "id": "9-2gWKVOw9pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read labels.txt and traina and test parquet"
      ],
      "metadata": {
        "id": "dlysu66Ww42s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK-E6raIjzEp"
      },
      "outputs": [],
      "source": [
        "LABELS = []\n",
        "\n",
        "train_dict = {}\n",
        "test_dict = {}\n",
        "\n",
        "# Obtain the list of labels\n",
        "with open(f'{dataset_path}/labels.txt') as f_labels:\n",
        "    LABELS = f_labels.read().splitlines()\n",
        "\n",
        "train_df = pd.read_parquet(f'{dataset_path}/train.parquet') # Read the parquet train dataset\n",
        "for k,v in tqdm(train_df.T.items(), total=len(list(train_df.T.items())), desc='Populating train dictionary'):\n",
        "    values = v.to_numpy()\n",
        "    train_dict[values[0]] = list(values[1:]) # Populate the train dictionary\n",
        "\n",
        "test_df = pd.read_parquet(f'{dataset_path}/test.parquet') # Read the parquet test dataset\n",
        "for k,v in tqdm(test_df.T.items(), total=len(list(test_df.T.items())), desc='Populating test dictionary'):\n",
        "    values = v.to_numpy()\n",
        "    test_dict[values[0]] = list(values[1:]) # Populate the test dictionary\n",
        "\n",
        "print(f'Sampled {percentage_train_test} % of Train & Test datasets, updated length: ', len(train_df), len(test_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC_q1x0-fjLN"
      },
      "source": [
        "# Neural Networks Part"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the *torchmetrics* package to use the F1-score and *torch_xla* for TPU usage"
      ],
      "metadata": {
        "id": "8nZcq42NAuHw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhfcTB21Q5YJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f79fc73-7a6b-43ac-d061-fb741050c92d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n",
            "Found existing installation: torch 1.13.1+cu116\n",
            "Uninstalling torch-1.13.1+cu116:\n",
            "  Successfully uninstalled torch-1.13.1+cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-xla==2.0\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl (115.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting torch==2.0.0\n",
            "  Downloading torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.1\n",
            "  Downloading torchvision-0.15.1-cp39-cp39-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.9/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 KB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0) (3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0\n",
            "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0) (4.5.0)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0) (3.10.2)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0) (3.1.2)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch==2.0.0) (1.11.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision==0.15.1) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision==0.15.1) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.15.1) (8.4.0)\n",
            "Collecting uritemplate<4dev,>=3.0.0\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.21.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.16.2)\n",
            "Collecting google-api-core<2dev,>=1.13.0\n",
            "  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.16.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.40.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (67.6.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch==2.0.0) (3.25.2)\n",
            "Collecting lit\n",
            "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from torch-xla==2.0) (1.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch==2.0.0) (2.1.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.9/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from oauth2client->cloud-tpu-client==0.10) (4.9)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from oauth2client->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.15.1) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.15.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.15.1) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.15.1) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.59.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /usr/local/lib/python3.9/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.19.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (5.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.9/dist-packages (from httplib2<1dev,>=0.9.2->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.9)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93601 sha256=290bb0509a0019044e759a08fe1ca8bf1ba87d080252c3bb5a40c3aca1d2cce8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/ee/80/1520ca86c3557f70e5504b802072f7fc3b0e2147f376b133ed\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, uritemplate, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, google-api-core, google-api-python-client, cloud-tpu-client, torch-xla, triton, torch, torchvision\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 4.1.1\n",
            "    Uninstalling uritemplate-4.1.1:\n",
            "      Successfully uninstalled uritemplate-4.1.1\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.11.0\n",
            "    Uninstalling google-api-core-2.11.0:\n",
            "      Successfully uninstalled google-api-core-2.11.0\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.70.0\n",
            "    Uninstalling google-api-python-client-2.70.0:\n",
            "      Successfully uninstalled google-api-python-client-2.70.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 2.0.0 which is incompatible.\n",
            "fastai 2.7.11 requires torch<1.14,>=1.7, but you have torch 2.0.0 which is incompatible.\n",
            "earthengine-api 0.1.346 requires google-api-python-client>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cloud-tpu-client-0.10 google-api-core-1.34.0 google-api-python-client-1.8.0 lit-16.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torch-xla-2.0 torchvision-0.15.1 triton-2.0.0 uritemplate-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics\n",
        "\n",
        "# In case you want to run the CNN part with the TPU instead of using a GPU you have to install torch_xla, a wrapper for pytorch that enable the usage of TPUs\n",
        "# since originally only Tensorflow can run in them\n",
        "\n",
        "!pip uninstall -y torch\n",
        "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl\n",
        "\n",
        "# Don't warry about the dependencies errors since we are using only torchvision and we don't care about torchaudio and torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnXsGuN8Ea0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2cb3be1-4198-48c2-edc7-5e680d2b747b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.0.0+cu117\n",
            "torchvision version: 0.15.1+cu117\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from timeit import default_timer as timer \n",
        "import copy\n",
        "from sklearn.metrics import f1_score  \n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torchmetrics.classification import MultilabelF1Score\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "print(f'PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDCWzDD4ExEk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "02c7a101-d329-44d8-93f1-ffd7a6f5a31f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "try:\n",
        "    os.environ['COLAB_TPU_ADDR']\n",
        "    device = xm.xla_device()\n",
        "except Exception as e:\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "device # Setup the device type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_mo3Ku6Gsud"
      },
      "outputs": [],
      "source": [
        "# Get the train and test path\n",
        "train_dir = dataset_path / 'train'\n",
        "test_dir = dataset_path / 'test'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial analysis"
      ],
      "metadata": {
        "id": "0POrdf00zQI7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF2sRptTfwS_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE: Obtaining the label name and index of a specific image\n",
        "\n",
        "TAKES:\n",
        "  - dictionary: is the dictionary from where we have to extract the ids\n",
        "  - path: describe the path of the specific bag of shots\n",
        "\n",
        "RETURNS: pairs of labels indices and names\n",
        "'''\n",
        "\n",
        "def get_label_ids_names(dictionary, path):\n",
        "    #print(dictionary[path])\n",
        "    ids = np.where(np.asarray(dictionary[path]) == 1)[0]\n",
        "    names = [LABELS[id] for id in ids]\n",
        "    return ids, names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ud9qlogG654"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE: Plot a random dataset image with its informations\n",
        "\n",
        "TAKES:\n",
        "  - dictionary: is the dictionary from where we have to extract the ids\n",
        "  - image_path_list: describe the path where we have to sample a image\n",
        "\n",
        "RETURNS: None\n",
        "'''\n",
        "\n",
        "def plot_random_image(dictionary, image_path_list):\n",
        "    random_image_path = random.choice(image_path_list) # Sample a image\n",
        "\n",
        "    path = os.path.relpath(random_image_path.parent, dataset_path) # Get the relative path\n",
        "    \n",
        "    image_classes_name, image_classes_id = get_label_ids_names(dictionary, path)\n",
        "    img = Image.open(random_image_path) # Open the image\n",
        "\n",
        "    print(f'Random image path: {random_image_path}')\n",
        "    print(f'Image classes id: {image_classes_id}')\n",
        "    print(f'Image classes name: {image_classes_name}')\n",
        "    print(f'Image height: {img.height}') \n",
        "    print(f'Image width: {img.width}')\n",
        "\n",
        "    #return img\n",
        "\n",
        "plot_random_image(train_dict, list(train_dir.glob('*/*/*.png')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdHloqARHVoq"
      },
      "source": [
        "Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYxaxNDTHXVi"
      },
      "outputs": [],
      "source": [
        "# Setting up the train and test transofms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.4588,0.4588,0.4588], std=[0.4588,0.4588,0.4588])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.4588,0.4588,0.4588], std=[0.4588,0.4588,0.4588])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Dataset to Dataloader"
      ],
      "metadata": {
        "id": "2LT7QuxhzKMV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_qzUtoXJGY9"
      },
      "source": [
        "Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRzgmcBbBPYt"
      },
      "outputs": [],
      "source": [
        "# VideoDataset object \n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, df, transform = None, t = 'single'):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.t = t\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.transform is None: self.transform = transforms.ToTensor()\n",
        "        images_path = self.df.iloc[index, 0]\n",
        "\n",
        "        shots = self.df.iloc[index, -1] # Get the number of frames of a bag of shots\n",
        "        # Each bag is half second frames\n",
        "\n",
        "        if self.t == 'single':\n",
        "            # I want only the central frame\n",
        "            images = self.transform(Image.open(f'{dataset_path}/{images_path}/shot{shots//2}.png'))\n",
        "\n",
        "        if self.t == 'early':\n",
        "            # I want the 5 middle frames\n",
        "            images = np.array([self.transform(Image.open(f'{dataset_path}/{images_path}/shot{idx}.png')).numpy() for idx in range((shots//2)-2,(shots//2)+3)])\n",
        "        \n",
        "        if self.t == 'late':\n",
        "            # I want the first and last frames\n",
        "            images = np.array([self.transform(Image.open(f'{dataset_path}/{images_path}/shot0.png')).numpy(), self.transform(Image.open(f'{dataset_path}/{images_path}/shot{shots-1}.png')).numpy()])\n",
        "        \n",
        "        if self.t == 'slow':\n",
        "            # I want the 10 middle frames\n",
        "            if shots%10 == 0:\n",
        "                images = np.array([self.transform(Image.open(f'{dataset_path}/{images_path}/shot{idx}.png')).numpy() for idx in range((shots/2) - 5, (shots/2) + 5)])\n",
        "            else:\n",
        "                images = np.array([self.transform(Image.open(f'{dataset_path}/{images_path}/shot{idx}.png')).numpy() for idx in range((shots%10) - (shots%10)//2, shots-(shots%10)//2)])\n",
        "\n",
        "        y_labels = torch.from_numpy(self.df.iloc[0, 1:-1].to_numpy().astype(float)) # Get the labels\n",
        "\n",
        "        if self.t != 'single': images = torch.from_numpy(images) # Convert the numpy image to tensor in case I want the central frame\n",
        "\n",
        "        return images, y_labels, images_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adtfogD8K0xz"
      },
      "source": [
        "Split the training set into train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fki8FMP_K0gZ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE: Splot the dataset in train and validation set\n",
        "\n",
        "TAKES:\n",
        "  - train_data: dataset to split\n",
        "  - perc_val_size: percentage of split\n",
        "\n",
        "RETURNS: train dataset and validation dataset\n",
        "'''\n",
        "\n",
        "def spit_train(train_data, perc_val_size):\n",
        "    train_size = len(train_data)\n",
        "    val_size = int((train_size * perc_val_size) // 100)\n",
        "    train_size -= val_size\n",
        "\n",
        "    return random_split(train_data, [int(train_size), int(val_size)]) #train_data, val_data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9xtNId8vjFa"
      },
      "source": [
        "Dataset definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX_hl5ESKuj7"
      },
      "outputs": [],
      "source": [
        "# Let's set the validations set to the 20% of the train dataset\n",
        "\n",
        "# Dataset for Single Frame\n",
        "train_data_single, val_data_single = spit_train(VideoDataset(df=train_df, transform=train_transform, t='single'), 20)\n",
        "test_data_single = VideoDataset(df=test_df, transform=test_transform, t='single')\n",
        "\n",
        "# Dataset for Multi Frame - Early Fusion\n",
        "train_data_early, val_data_early = spit_train(VideoDataset(df=train_df, transform=train_transform, t='early'), 20)\n",
        "test_data_early = VideoDataset(df=test_df, transform=test_transform, t='early')\n",
        "\n",
        "# Dataset for Multi Frame - Late Fusion\n",
        "train_data_late, val_data_late = spit_train(VideoDataset(df=train_df, transform=train_transform, t='late'), 20)\n",
        "test_data_late = VideoDataset(df=test_df, transform=test_transform, t='late')\n",
        "\n",
        "# Dataset for Multi Frame - Slow Fusion\n",
        "train_data_slow, val_data_slow = spit_train(VideoDataset(df=train_df, transform=train_transform, t='slow'), 20)\n",
        "test_data_slow = VideoDataset(df=test_df, transform=test_transform, t='slow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wymzpd8yfBAU"
      },
      "source": [
        "### Random Image from the train and test dataloader for Single Frame model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PORPOUSE: Plot a random image from a given dataset\n",
        "\n",
        "TAKES:\n",
        "  - dt: dataset where sample a image\n",
        "  - dic: dictionary to which the image belongs\n",
        "\n",
        "RETURNS: train dataset and validation dataset\n",
        "'''\n",
        "\n",
        "def random_image_from_dataloader(dt, dic):\n",
        "    rnd = random.choice(dt)\n",
        "    img, label, path = rnd\n",
        "    label_ids, label_names = get_label_ids_names(dic, path)\n",
        "\n",
        "    print(f'Image tensor:\\n{img}')\n",
        "    print(f'Image shape: {img.shape}')\n",
        "    print(f'Image datatype: {img.dtype}')\n",
        "    print(f'Image label ids: {label_ids}')\n",
        "    print(f'Image label names: {label_names}')\n",
        "    print(f'Label datatype: {type(label)}')\n",
        "    plt.imshow(img.permute(1, 2, 0).numpy())\n",
        "    plt.axis(False)"
      ],
      "metadata": {
        "id": "CKsG07QC_y4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGF5v86820RF"
      },
      "outputs": [],
      "source": [
        "random_image_from_dataloader(train_data_single, train_dict) # Random image from the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwqji2FGb_El"
      },
      "outputs": [],
      "source": [
        "random_image_from_dataloader(test_data_single, test_dict) # Random image from the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A9MPVAYM4hi"
      },
      "source": [
        "\n",
        "### Turn loaded images into DataLoader's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp3ZwwtosfCP"
      },
      "outputs": [],
      "source": [
        "# Turn train and test satasets into DataLoaders\n",
        "BATCH_SIZE = 8 # <---------------------------- TODO: tune this paramenter\n",
        "NUM_WORKERS = os.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SomP75Zsd0U"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE: generate the train vaiadtion and test dataloaders\n",
        "\n",
        "TAKES:\n",
        "  - train_data: train dataset\n",
        "  - val_data: validation dataset\n",
        "  - test_data: test dataset\n",
        "  - batch_size: specify how many bags of shots a bach must have\n",
        "  - num_workers: specify how many workers will work on the dataloaders creation \n",
        "\n",
        "RETURNS: train, val and test dataloaders\n",
        "'''\n",
        "\n",
        "def generate_dataloaders(train_data, val_data, test_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
        "\n",
        "    train_dl = DataLoader(dataset = train_data, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle = True)\n",
        "    val_dl = DataLoader(dataset = val_data, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle = True)\n",
        "    test_dl = DataLoader(dataset = test_data, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle = False)\n",
        "\n",
        "    return train_dl, val_dl, test_dl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfHVNSmHuZsb"
      },
      "source": [
        "## Models definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk_25zkeucFh"
      },
      "outputs": [],
      "source": [
        "# Defining the main custom Alexnet that ereditate the fucntion of nn.Module\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, in_channels, stream_type=None, t_frames=[1,1,1]):\n",
        "        # stream_type: none, \"context\", \"fovea\"\n",
        "        # None is used for considering the cnn with spartial fusion informations\n",
        "        super().__init__()\n",
        "\n",
        "        # input size nomultiresulution : (b x 3 x 170 x 170)\n",
        "        # input size multiresulution : (b x 3 x 178 x 178) take into account the video classifier -> which becomes 89 x 89 in both streams\n",
        "\n",
        "        self.stream_type = stream_type\n",
        "        self.fovea = transforms.Compose([transforms.CenterCrop((89, 89))])\n",
        "        self.context = transforms.Compose([transforms.Resize((89, 89))])\n",
        "        self.transform = transforms.Compose([transforms.Resize((170, 170))])\n",
        "        self.t_frames = t_frames\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*self.t_frames[0], 96, kernel_size=11, stride=3, padding=2),  \n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 96 x 170 x 170) -> (b x 96 x 55 x 55)\n",
        "            # MULTIRES -> in: (b x 96 x 89 x 89) -> (b x 96 x 28 x 28)\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.5, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 96 x 55 x 55) -> (b x 96 x 28 x 28)\n",
        "            # MULTIRES -> in: (b x 96 x 28 x 28) -> (b x 96 x 14 x 14)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(96*self.t_frames[1], 256, kernel_size=5, stride=1, padding=2), \n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 96 x 28 x 28) -> (b x 256 x 28 x 28)\n",
        "            # MULTIRES -> in: (b x 96 x 14 x 14) -> (b x 256 x 14 x 14)\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.5, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 256 x 28 x 28) -> (b x 256 x 14 x 14)\n",
        "            # MULTIRES -> in: (b x 256 x 14 x 14) -> (b x 256 x 7 x 7)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256*self.t_frames[2], 384, kernel_size=3, stride=1, padding=1), \n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 256 x 14 x 14) -> (b x 384 x 14 x 14)\n",
        "            # MULTIRES -> in: (b x 256 x 7 x 7) -> (b x 384 x 7 x 7)\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 384 x 14 x 14) -> (b x 384 x 14 x 14)\n",
        "            # MULTIRES -> in: (b x 384 x 7 x 7) -> (b x 384 x 7 x 7)\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 384 x 14 x 14) -> (b x 256 x 14 x 14)\n",
        "            # MULTIRES -> in: (b x 384 x 7 x 7) -> (b x 256 x 7 x 7)\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.MaxPool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # CORRECT\n",
        "        # NO-MULTIRES -> in: (b x 256 x 14 x 14) -> (b x 256 x 7 x 7)\n",
        "\n",
        "        self.init_bias()  # initialize bias -> CHECK IF IT MAKE SENSE\n",
        "\n",
        "    def init_bias(self):\n",
        "        for block in [self.conv1, self.conv2, self.conv3]:\n",
        "            for layer in block:\n",
        "                if isinstance(layer, nn.Conv2d):\n",
        "                    nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "                    nn.init.constant_(layer.bias, 0)\n",
        "        # original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers\n",
        "        nn.init.constant_(self.conv2[0].bias, 1)\n",
        "        nn.init.constant_(self.conv3[2].bias, 1)\n",
        "        nn.init.constant_(self.conv3[4].bias, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.stream_type != None:\n",
        "            x = self.fovea(x) if self.stream_type == 'fovea' else self.context(x)\n",
        "            return self.conv3(self.conv2(self.conv1(x)))\n",
        "        else: \n",
        "            x = self.transform(x)\n",
        "            return self.MaxPool(self.conv3(self.conv2(self.conv1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRVs6r74wQDu"
      },
      "source": [
        "### Single frame CNNs:\n",
        "1. Single-Fame (custom AlexNet)\n",
        "2. Single-Fame Fovea Only\n",
        "3. Single-Fame Context Only\n",
        "4. Single-Fame + Multires\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJYGNCZWvvCp"
      },
      "outputs": [],
      "source": [
        "# The single frame CNN, includes the three CNNs from the above list\n",
        "\n",
        "class NoMultiresCNN(nn.Module):\n",
        "    def __init__(self, CNN, num_classes):\n",
        "        super(NoMultiresCNN, self).__init__()\n",
        "        self.CNN = CNN\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=(256 * 7 * 7), out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.CNN(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obJWjr0qvtYd"
      },
      "outputs": [],
      "source": [
        "# Defining the single frame multiresolution CNN \n",
        "\n",
        "class MultiResolutionCNN(nn.Module):\n",
        "    def __init__(self, AlexNet_fovea, AlexNet_context, num_classes):\n",
        "        super(MultiResolutionCNN, self).__init__()\n",
        "        self.AlexNet_fovea = AlexNet_fovea\n",
        "        self.AlexNet_context = AlexNet_context\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=(512 * 7 * 7), out_features=4096), # 512 since it takes the double of the infrormations\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x1 = self.AlexNet_fovea(x.clone()) # Takes the fovea strea\n",
        "        x2 = self.AlexNet_context(x.clone()) # Takes the contxt stream\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZGMVBhfbwb9"
      },
      "source": [
        "### Multi Frames CNNs\n",
        "1. Late Fusion\n",
        "2. Early Fusion\n",
        "3. Slow Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lj0WeGNvwmh"
      },
      "outputs": [],
      "source": [
        "# Late Fusion Model\n",
        "\n",
        "class LateFusionCNN(nn.Module):\n",
        "    def __init__(self, AlexNet_1, AlexNet_2, num_classes):\n",
        "        super(LateFusionCNN, self).__init__()\n",
        "        self.AlexNet_1 = AlexNet_1 # Two Separates AlexNet\n",
        "        self.AlexNet_2 = AlexNet_2\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=(512 * 7 * 7), out_features=4096),  # 512 since it takes the double of the infrormations\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        initial = torch.empty(x.shape[0], 3, 178, 178).to(device)\n",
        "        final = torch.empty(x.shape[0], 3, 178, 178).to(device)\n",
        "        \n",
        "        for i in range(x.shape[0]):\n",
        "            initial[i] = x[i][0]\n",
        "            final[i] = x[i][1]\n",
        "\n",
        "        initial = self.AlexNet_1(initial)\n",
        "        final = self.AlexNet_2(final)\n",
        "        x = torch.cat((initial, final), dim=1)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzk4JoNYvyCW"
      },
      "outputs": [],
      "source": [
        "# Early Fusion Model\n",
        "\n",
        "class EarlyFusionCNN(nn.Module):\n",
        "    def __init__(self, AlexNet, num_classes):\n",
        "        super(EarlyFusionCNN, self).__init__()\n",
        "        self.AlexNet = AlexNet\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=(256 * 7 * 7), out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.reshape(x.shape[0], x.shape[1]*3, 178, 178)\n",
        "        x = self.AlexNet(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx_BjklmvzeU"
      },
      "outputs": [],
      "source": [
        "# Sow Fusion Model\n",
        "\n",
        "class SlowFusionCNN(nn.Module):\n",
        "    def __init__(self, AlexNet, num_classes):\n",
        "        super(SlowFusionCNN, self).__init__()\n",
        "        self.AlexNet = AlexNet\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=(256 * 7 * 7), out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        bag = torch.empty(4,x.shape[0],3*self.AlexNet.t_frames[0],178,178).to(device)\n",
        "        \n",
        "        for idx in range(bag.shape[0]): # 0 - 4\n",
        "            for i in range(x.shape[0]): # 0 - batch_size\n",
        "                bag[idx][i] = x[i][(idx) : (idx+4)].reshape(3*self.AlexNet.t_frames[0],178,178)\n",
        "\n",
        "        rconv2_2 = torch.cat((self.AlexNet.conv2( # Second layer\n",
        "            torch.cat((self.AlexNet.conv1(bag[0]), self.AlexNet.conv1(bag[1])), dim=1) # First layer\n",
        "        ), self.AlexNet.conv2( # Second layer\n",
        "            torch.cat((self.AlexNet.conv1(bag[2]), self.AlexNet.conv1(bag[3])), dim=1) # First layer\n",
        "        )), dim=1)\n",
        "\n",
        "        x = self.AlexNet.MaxPool(self.AlexNet.conv3(rconv2_2)) # Third layer\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPU84FGDFw5G"
      },
      "source": [
        "### Train and Evaluate Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L43iesXbv04o"
      },
      "outputs": [],
      "source": [
        "# CNN Architecture to perform the Train and Evaluation steps saving the results\n",
        "\n",
        "class CNN_Architecture():\n",
        "\n",
        "  def __init__(self, model: torch.nn.Module, train_dataloader: torch.utils.data.DataLoader, \n",
        "      val_dataloader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer,\n",
        "      loss_fn: torch.nn.Module, accuracy_fn, scheduler: torch.optim.Optimizer, device: torch.device, save_check = False):\n",
        "\n",
        "      self.model = model.to(device)\n",
        "      self.optimizer = optimizer\n",
        "      self.train_dataloader = train_dataloader\n",
        "      self.loss_fn = loss_fn\n",
        "      self.val_dataloader = val_dataloader\n",
        "      self.accuracy_fn = accuracy_fn\n",
        "      self.scheduler = scheduler\n",
        "      self.device = device\n",
        "      self.save_check = save_check\n",
        "\n",
        "\n",
        "  '''\n",
        "  PORPOUSE: save the a checkpoint model\n",
        "\n",
        "  TAKES:\n",
        "    - train_loss: actual train loss\n",
        "    - train_f1: actual train f1 score\n",
        "    - epoch\n",
        "\n",
        "  RETURNS: None\n",
        "  '''\n",
        "  def __save_checkpoint(self, train_loss, train_f1, epoch):\n",
        "      data_path = Path('data/')\n",
        "      filename = f'{self.model.typ}_checkpoint.pth.tar'\n",
        "      print('=> Saving Checkpoint')\n",
        "      checkpoint = {'state_dict': self.model.state_dict(), 'optimizer': self.optimizer.state_dict(),\n",
        "                    'train_loss': train_loss, 'train_f1': train_f1, 'epoch': epoch}\n",
        "      torch.save(checkpoint, filename)\n",
        "      print(' DONE\\n')\n",
        "\n",
        "\n",
        "  '''\n",
        "  PORPOUSE: Load a chekpoint\n",
        "\n",
        "  TAKES:\n",
        "    - checkpoint: checkpoint to load\n",
        "\n",
        "  RETURNS: None\n",
        "  '''\n",
        "  def __load_checkpoint(self,checkpoint):\n",
        "      self.model.load_state_dict(checkpoint['state_dict'])\n",
        "      self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  \n",
        "\n",
        "  '''\n",
        "  PORPOUSE: Perform the model evaluation / testing\n",
        "\n",
        "  TAKES:\n",
        "    - val_dataloader: checkpoint to load\n",
        "    - epoch / epochs\n",
        "\n",
        "  RETURNS: dictionary containing the model name, loss and socore\n",
        "  '''\n",
        "  def evaluate(self, val_dataloader: torch.utils.data.DataLoader, epoch = 0, epochs = 1):\n",
        "      val_loss, val_f1 = 0, 0\n",
        "\n",
        "      self.model.eval() # Evaluation phase\n",
        "\n",
        "      pbar = tqdm(enumerate(val_dataloader), total = len(val_dataloader), leave=False) # Initialize the progress bar\n",
        "\n",
        "      with torch.inference_mode(): # Allow inference mode\n",
        "          for batch_idx, (images, labels, _) in pbar:\n",
        "              images, labels = images.to(self.device), labels.to(self.device) # Move the images and labels into the device\n",
        "\n",
        "              outputs = self.model(images) # Get the model output\n",
        "\n",
        "              loss = self.loss_fn(outputs, labels) # Get the loss\n",
        "\n",
        "              f1 = self.accuracy_fn(outputs, labels).item() # Perform the score\n",
        "\n",
        "              # Increment the statistics\n",
        "              val_loss += loss.item()\n",
        "              val_f1 += f1\n",
        "\n",
        "              # Update the progress bar\n",
        "              if epoch > 0: pbar.set_description(f'{self.model.__class__.__name__} EVALUATION Epoch [{epoch + 1} / {epochs}]')\n",
        "              else: pbar.set_description(f'{self.model.__class__.__name__} TESTING')\n",
        "              pbar.set_postfix(loss = loss.item(), f1 = f1)\n",
        "            \n",
        "          val_loss /= len(val_dataloader) # Calculate the final loss\n",
        "          val_f1 /= len(val_dataloader) # Calculate the final score\n",
        "\n",
        "      model_name = self.model.__class__.__name__\n",
        "      if self.model.__class__.__name__ == 'NoMultiresCNN': model_name = f'{model_name} - Stream Type: {self.model.CNN.stream_type}'\n",
        "\n",
        "      return { 'model_name': model_name,\n",
        "              'model_loss': val_loss,\n",
        "              'model_f1': val_f1 }\n",
        "\n",
        "\n",
        "  '''\n",
        "  PORPOUSE: Perform the model traing\n",
        "\n",
        "  TAKES:\n",
        "    - epochs: number of times that our model will see the data\n",
        "\n",
        "  RETURNS: dictionary of results containing model name and history of results for each epoch\n",
        "  '''\n",
        "  def fit(self, epochs: int):\n",
        "      results = { 'train_loss': [], 'train_f1': [], 'val_loss': [], 'val_f1': [] }\n",
        "      best_train_loss, best_train_f1 = float('inf'), float('-inf')\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          train_loss, train_f1 = 0, 0\n",
        "\n",
        "          pbar = tqdm(enumerate(self.train_dataloader), total = len(self.train_dataloader), leave=False) # Initialize the progress bar\n",
        "          \n",
        "          for batch_idx, (images, labels, _) in pbar:\n",
        "\n",
        "              self.model.train() # Training phase\n",
        "\n",
        "              # zero_grad -> backword -> step\n",
        "\n",
        "              self.optimizer.zero_grad()\n",
        "              images, labels = images.to(self.device), labels.to(self.device) # Move the images and labels into the device\n",
        "              \n",
        "              outputs = self.model(images) # Get the model output\n",
        "\n",
        "              loss = self.loss_fn(outputs, labels) # Get the loss\n",
        "\n",
        "              loss.backward() # Backword step\n",
        "              self.optimizer.step()\n",
        "\n",
        "              train_loss += loss.item()\n",
        "              f1 = self.accuracy_fn(outputs, labels).item() # Perform the score\n",
        "\n",
        "              train_f1 += f1\n",
        "\n",
        "              model_name = self.model.__class__.__name__\n",
        "              if self.model.__class__.__name__ == 'NoMultiresCNN': model_name = f'{model_name} - Stream Type: {self.model.CNN.stream_type}'\n",
        "\n",
        "              # Update the progress bar\n",
        "              pbar.set_description(f'{model_name} TRAIN Epoch [{epoch + 1} / {epochs}]')\n",
        "              pbar.set_postfix(loss = loss.item(), f1 = f1)\n",
        "\n",
        "\n",
        "          train_loss /= len(self.train_dataloader) # Calculate the final loss\n",
        "          train_f1 /= len(self.train_dataloader) # Calculate the final score\n",
        "\n",
        "\n",
        "          self.scheduler.step(train_loss)\n",
        "\n",
        "          if(self.save_check): # Save a checkpoint if the actual results are the best one \n",
        "              if(train_loss < best_train_loss and train_f1 > best_train_f1):\n",
        "                  self.__save_checkpoint(train_loss, train_f1, epoch + 1)\n",
        "                  best_train_loss, best_train_f1 = train_loss, train_f1\n",
        "\n",
        "          # Validation phase\n",
        "          model_name, val_loss, val_f1 = (self.evaluate(self.val_dataloader, epoch, epochs)).values()\n",
        "\n",
        "          # Append the results of the current epoch\n",
        "          results['train_loss'].append(train_loss)\n",
        "          results['train_f1'].append(train_f1)\n",
        "          results['val_loss'].append(val_loss)\n",
        "          results['val_f1'].append(val_f1)\n",
        "\n",
        "          print('Epoch [{}], train_loss: {:.6f}, train_f1: {:.6f}, val_loss: {:.6f}, val_f1: {:.6f} \\n'.format(\n",
        "                epoch + 1, train_loss, train_f1, val_loss, val_f1))\n",
        "\n",
        "      return {'model_name': model_name, 'results': results}\n",
        "\n",
        "  \n",
        "  '''\n",
        "  PORPOUSE: Perform the evaluation of an image and plot it\n",
        "\n",
        "  TAKES:\n",
        "    - image_tensor: tensor representing the image \n",
        "    - class_names: labels name of our image\n",
        "    - transform\n",
        "    - mean\n",
        "    - std\n",
        "\n",
        "  RETURNS: None\n",
        "  '''\n",
        "  def evaluate_and_plot_image(self, image_tensor, class_names, transform=None, mean=[0.4588,0.4588,0.4588], std=[0.4588,0.4588,0.4588]):\n",
        "      topk = 3 # Number of top k labels that we want to see\n",
        "\n",
        "      if transform is not None: image_transform = transform\n",
        "      else: image_transform = transforms.Compose([ transforms.Normalize(mean=mean, std=std) ])\n",
        "      \n",
        "      self.model.to(self.device) # Move the model to device\n",
        "\n",
        "      self.model.eval() # Evauation phase\n",
        "      \n",
        "      with torch.inference_mode(): # Allow inference mode\n",
        "          transformed_image = image_transform(image_tensor).unsqueeze(dim=0)\n",
        "          targets_image_pred = self.model(transformed_image.to(self.device)) # Get the model output\n",
        "\n",
        "      target_image_pred_probs = torch.sigmoid(targets_image_pred).to('cpu')\n",
        "      top_k_probs_labels = torch.topk(target_image_pred_probs, k=topk, dim=1) # Get the probability of the top K labels\n",
        "\n",
        "      label_pred_names = [LABELS[int(lab.numpy())]for lab in top_k_probs_labels.indices[0]] # Get the name of the top K labels\n",
        "\n",
        "      # Image plot\n",
        "      plt.figure()\n",
        "      plt.imshow(image_tensor.permute(1, 2, 0).numpy())\n",
        "      plt.title(f\"Model: {self.model.__class__.__name__}\")\n",
        "      plt.axis(False)\n",
        "\n",
        "      print(f'True Labels:\\t{class_names}\\nTop {topk} Pred:\\t{label_pred_names}\\nTop {topk} Prob:\\t{top_k_probs_labels.values[0].numpy().tolist()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0Qg5iACv3MV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE: Perform MultilabelF1Score\n",
        "\n",
        "TAKES:\n",
        "  - outputs: model output\n",
        "  - labels: ground truth\n",
        "\n",
        "RETURNS: MultilabelF1Score\n",
        "'''\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    metric = MultilabelF1Score(num_labels=len(LABELS)).to(device)\n",
        "    return metric(outputs, labels)\n",
        "    # seems reasonable, for me not at all, always 0.00205!!!!!!!!!!!!!!!!!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztfRyKzxF64O"
      },
      "source": [
        "# Let's run all!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOGOoENIwmIM"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 5\n",
        "n_classes = len(LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpeIH64PUI7r"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE:\n",
        "  Plot Loss and Accuracy curves\n",
        "\n",
        "TAKES:\n",
        "  - results_info: dictionary containing the results and model name\n",
        "\n",
        "RETURNS:\n",
        "  None\n",
        "'''\n",
        "\n",
        "def plot_loss_curves(results_info):\n",
        "    res = results_info['results']\n",
        "    epochs = range(len(res['train_loss']))\n",
        "\n",
        "    plt.figure(figsize = (15, 7))\n",
        "    plt.title(results_info['model_name'])\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, res['train_loss'], label = 'train_loss')\n",
        "    plt.plot(epochs, res['val_loss'], label = 'val_loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, res['train_f1'], label = 'train_f1_accuracy')\n",
        "    plt.plot(epochs, res['val_f1'], label = 'val_f1_accuracy')\n",
        "    plt.title('F1 Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjnnZL4XUD1x"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE:\n",
        "  Perform the training and evaluation plotting the results\n",
        "\n",
        "TAKES:\n",
        "  - model: CNN model to use\n",
        "  - test: test dataloader\n",
        "  - EPOCHS: numer of times that our model will see the data\n",
        "\n",
        "RETURNS:\n",
        "  None (results)\n",
        "'''\n",
        "\n",
        "def train_evaluate(model, test, epochs=NUM_EPOCHS):\n",
        "    start_time = timer()\n",
        "    history = model.fit(NUM_EPOCHS)  # Train the model, it returns {'model_name': model_name, 'results': results}\n",
        "    end_time = timer()\n",
        "\n",
        "    print(f'Total training time: {end_time-start_time:.3f} seconds')\n",
        "\n",
        "    plot_loss_curves(history) # Compare the results between train and validation set\n",
        "\n",
        "    start_time = timer()\n",
        "    result = model.evaluate(test) # Evaluate the model in the Tran dataloader\n",
        "    # It returns {'model_name': model_name, 'model_loss': val_loss.item(), 'model_f1': val_f1.item()}\n",
        "    end_time = timer()\n",
        "\n",
        "\n",
        "    print(f'Total evaluation time: {end_time-start_time:.3f} seconds\\n')\n",
        "    print(f\"TEST Results for {result['model_name']} -> loss: {result['model_loss']} f1-accuracy: {result['model_f1']}\")\n",
        "    \n",
        "    # lo ritorno quanfo tutto funziona\n",
        "    #return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR3T8ZsbR7bW"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE:\n",
        "  Perform the evaluation given a single image\n",
        "\n",
        "TAKES:\n",
        "  - dl: image dataloader\n",
        "  - dic: image dictionary\n",
        "  - cnn_arc: CNN model\n",
        "\n",
        "RETURNS:\n",
        "  None (results)\n",
        "'''\n",
        "\n",
        "def test_single_image(dl, dic, cnn_arc):\n",
        "    rnd = random.choice(dl)\n",
        "    img, label, path = rnd\n",
        "    label_ids, label_names = get_label_ids_names(dic, path)\n",
        "    cnn_arc.evaluate_and_plot_image(img, label_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8zU0Dr3zjuB"
      },
      "outputs": [],
      "source": [
        "# Calculate the class weights since we have an imbalance dataset\n",
        "\n",
        "# La divisione per zero c'è perchè non ho abbastanza dati\n",
        "\n",
        "#https://stackoverflow.com/questions/57021620/how-to-calculate-unbalanced-weights-for-bcewithlogitsloss-in-pytorch\n",
        "\n",
        "'''\n",
        "PORPOUSE:\n",
        "  Calculate the class weights since we have an imbalance dataset\n",
        "\n",
        "TAKES:\n",
        "  - train_ds: train dataframe\n",
        "  - test_ds: test dataframe\n",
        "  - n_classes: number of classes\n",
        "\n",
        "RETURNS:\n",
        "  np.array of weights\n",
        "'''\n",
        "\n",
        "def get_classes_weight(train_df, test_df, n_classes):\n",
        "    col = list(map(str, range(n_classes)))\n",
        "\n",
        "    count_train = train_df[col].sum(axis=0).to_numpy()\n",
        "    count_test = test_df[col].sum(axis=0).to_numpy()\n",
        "    count = count_train + count_test\n",
        "\n",
        "    # MOMENTANEAMENTE PER ESCLUDERE LE DIVISIONI PER ZERO\n",
        "    # ------------------- DA COMMENTARE DOPO AVER UN BUON DATASET -------------------\n",
        "    foo = np.array(count)\n",
        "    foo[foo == 0] = 1\n",
        "    # ------------------- DA COMMENTARE DOPO AVER UN BUON DATASET -------------------\n",
        "\n",
        "\n",
        "    l = len(train_df) + len(test_df)\n",
        "    full = torch.full((1,len(LABELS)), l).squeeze().numpy()\n",
        "\n",
        "    return torch.from_numpy((full - foo) / foo).to(device)\n",
        "\n",
        "weights = get_classes_weight(train_df, test_df, n_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloaders definitons"
      ],
      "metadata": {
        "id": "MBz1KYqz361D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLqx2oQoI6kM"
      },
      "outputs": [],
      "source": [
        "train_dl_single, val_dl_single, test_dl_single = generate_dataloaders(train_data_single, val_data_single, test_data_single)\n",
        "train_dl_late, val_dl_late, test_dl_late = generate_dataloaders(train_data_late, val_data_late, test_data_late)\n",
        "train_dl_early, val_dl_early, test_dl_early = generate_dataloaders(train_data_early, val_data_early, test_data_early)\n",
        "train_dl_slow, val_dl_slow, test_dl_slow = generate_dataloaders(train_data_slow, val_data_slow, test_data_slow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHy96tKkQ2da"
      },
      "source": [
        "## Single Frame Model CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9mt0-8fQ6OR"
      },
      "outputs": [],
      "source": [
        "AN_single = AlexNet(in_channels=3) # Extractor\n",
        "Single_CNN = NoMultiresCNN(AN_single, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_single = torch.optim.SGD(params=Single_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_single = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_single, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_single = CNN_Architecture(model = Single_CNN, \n",
        "             train_dataloader = train_dl_single,\n",
        "             val_dataloader = val_dl_single,\n",
        "             optimizer = optimizer_single,\n",
        "             loss_fn = nn.BCEWithLogitsLoss(pos_weight=weights),\n",
        "             accuracy_fn = accuracy,\n",
        "             scheduler = scheduler_single,\n",
        "             device = device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluate(CNN_arch_single, test_dl_single) # Run train and evaluation"
      ],
      "metadata": {
        "id": "UooZ50TaotSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_single_image(test_data_single, test_dict, CNN_arch_single) # Test in a single random image"
      ],
      "metadata": {
        "id": "pk6xVUEnkZdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65rlij38SdzA"
      },
      "source": [
        "## Single Frame Model Fovea CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asd7eEk9Sj9i"
      },
      "outputs": [],
      "source": [
        "AN_Fovea = AlexNet(in_channels=3, stream_type='fovea') # Extractor\n",
        "Fovea_CNN = NoMultiresCNN(AN_Fovea, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_fovea = torch.optim.SGD(params=Fovea_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_fovea = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_fovea, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_fovea = CNN_Architecture(model = Fovea_CNN, \n",
        "             train_dataloader = train_dl_single,\n",
        "             val_dataloader = val_dl_single,\n",
        "             optimizer = optimizer_fovea,\n",
        "             loss_fn = nn.BCEWithLogitsLoss(pos_weight=weights),\n",
        "             accuracy_fn = accuracy,\n",
        "             scheduler = scheduler_fovea,\n",
        "             device = device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluate(CNN_arch_fovea, test_dl_single) # Run train and evaluation"
      ],
      "metadata": {
        "id": "d6EK2UB6szaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_single_image(test_data_single, test_dict, CNN_arch_fovea) # Test in a single random image"
      ],
      "metadata": {
        "id": "GlnRA81VsyJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRbO3-84S-js"
      },
      "source": [
        "## Single Frame Model Context CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP00WwvQTD9N"
      },
      "outputs": [],
      "source": [
        "AN_Context = AlexNet(in_channels=3, stream_type='context') # Extractor\n",
        "Context_CNN = NoMultiresCNN(AN_Context, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_context = torch.optim.SGD(params=Context_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_context = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_context, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_context = CNN_Architecture(model = Context_CNN, \n",
        "             train_dataloader = train_dl_single,\n",
        "             val_dataloader = val_dl_single,\n",
        "             optimizer = optimizer_context,\n",
        "             loss_fn = nn.BCEWithLogitsLoss(pos_weight=weights),\n",
        "             accuracy_fn = accuracy,\n",
        "             scheduler = scheduler_context,\n",
        "             device = device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluate(CNN_arch_context, test_dl_single) # Run train and evaluation"
      ],
      "metadata": {
        "id": "OJIYUGJStEFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_single_image(test_data_single, test_dict, CNN_arch_context) # Test in a single random image"
      ],
      "metadata": {
        "id": "ZRuXA7uHtFwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh7rm0-PTUIK"
      },
      "source": [
        "## Single Frame Model Multiresolution CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m__Ss3cLTTtt"
      },
      "outputs": [],
      "source": [
        "AN_Multi_Fovea, AN_Multi_context = AlexNet(in_channels=3, stream_type='fovea'), AlexNet(in_channels=3, stream_type='context') # Extractors\n",
        "Multi_CNN = MultiResolutionCNN(AN_Multi_Fovea, AN_Multi_context, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_multi = torch.optim.SGD(params=Multi_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_multi = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_multi, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_multi = CNN_Architecture(model = Multi_CNN, \n",
        "             train_dataloader = train_dl_single,\n",
        "             val_dataloader = val_dl_single,\n",
        "             optimizer = optimizer_multi,\n",
        "             loss_fn = nn.BCEWithLogitsLoss(pos_weight=weights),\n",
        "             accuracy_fn = accuracy,\n",
        "             scheduler = scheduler_multi,\n",
        "             device = device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluate(CNN_arch_multi, test_dl_single) # Run train and evaluation"
      ],
      "metadata": {
        "id": "IdRfMVrhtIbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_single_image(test_data_single, test_dict, CNN_arch_multi) # Test in a single random image"
      ],
      "metadata": {
        "id": "aGfrZK99tJ6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gtKCo7fTidh"
      },
      "source": [
        "## Multi Frame Model Late Fusion CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZzRfYX3TrEN"
      },
      "outputs": [],
      "source": [
        "AN_Late1, AN_late2 = AlexNet(in_channels=3), AlexNet(in_channels=3) # Extractors\n",
        "Late_CNN = LateFusionCNN(AN_Late1, AN_late2, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_late = torch.optim.SGD(params=Late_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_late = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_late, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_late = CNN_Architecture(model = Late_CNN, \n",
        "             train_dataloader = train_dl_late,\n",
        "             val_dataloader = val_dl_late,\n",
        "             optimizer = optimizer_late,\n",
        "             loss_fn = nn.BCEWithLogitsLoss(pos_weight=weights),\n",
        "             accuracy_fn = accuracy,\n",
        "             scheduler = scheduler_late,\n",
        "             device = device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluate(CNN_arch_late, test_dl_late) # Run train and evaluation"
      ],
      "metadata": {
        "id": "5Dw4PGEztONY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_single_image(test_data_late, test_dict, CNN_arch_late) # Test in a single random image"
      ],
      "metadata": {
        "id": "zoZiZLprtL1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqYxdZBbUfqU"
      },
      "source": [
        "## Multi Frame Model Early Fusion CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjP_CL0JUDLR"
      },
      "outputs": [],
      "source": [
        "AN_Early = AlexNet(in_channels=3, t_frames=[5,1,1]) # Extractor\n",
        "Early_CNN = EarlyFusionCNN(AN_Early, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_early = torch.optim.SGD(params=Early_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_early = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_early, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_early = CNN_Architecture(model = Early_CNN, \n",
        "             train_dataloader = train_dl_early,\n",
        "             val_dataloader = val_dl_early,\n",
        "             optimizer = optimizer_early,\n",
        "             loss_fn = nn.BCEWithLogitsLoss(pos_weight=weights),\n",
        "             accuracy_fn = accuracy,\n",
        "             scheduler = scheduler_early,\n",
        "             device = device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluate(CNN_arch_early, test_dl_early) # Run train and evaluation"
      ],
      "metadata": {
        "id": "FDB2D8BbtRxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_single_image(test_data_early, test_dict, CNN_arch_early) # Test in a single random image"
      ],
      "metadata": {
        "id": "-nYUZ8PXtTiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGmsIQOUqPp"
      },
      "source": [
        "## Multi Frame Model Slow Fusion CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO8Qvod3Usw3"
      },
      "outputs": [],
      "source": [
        "AN_Slow = AlexNet(in_channels=3, t_frames=[4,2,2]) # Extractor\n",
        "Slow_CNN = SlowFusionCNN(AN_Slow, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_slow = torch.optim.SGD(params=Slow_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_slow = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_slow, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_slow = CNN_Architecture(model = Slow_CNN, \n",
        "             train_dataloader = train_dl_slow,\n",
        "             val_dataloader = val_dl_slow,\n",
        "             optimizer = optimizer_slow,\n",
        "             loss_fn = nn.BCEWithLogitsLoss(pos_weight=weights),\n",
        "             accuracy_fn = accuracy,\n",
        "             scheduler = scheduler_slow,\n",
        "             device = device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_evaluate(CNN_arch_slow, test_dl_slow) # Run train and evaluation"
      ],
      "metadata": {
        "id": "cXPaEJXltVEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_single_image(test_data_slow, test_dict, CNN_arch_slow) # Test in a single random image"
      ],
      "metadata": {
        "id": "ndK5ZImttXJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "422ae7830e3740caa81197397880bd39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13e906e348e44ea19febd28714295e58",
              "IPY_MODEL_f2af354e9ad24460aae635f52e98dc3d",
              "IPY_MODEL_4770452fcc8d485c8ab8dae12c7937f9"
            ],
            "layout": "IPY_MODEL_51e40810a55d4ff587b4ec5452a5776e"
          }
        },
        "13e906e348e44ea19febd28714295e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd1ab633a60f438e96549fb829066b30",
            "placeholder": "​",
            "style": "IPY_MODEL_b1235c12bfcc4c3a9926e0216a533a19",
            "value": "  1%"
          }
        },
        "f2af354e9ad24460aae635f52e98dc3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb79c74d916d42799dd7ac341fa69b0a",
            "max": 1946590,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b271af095877404dac02f67142e56ca8",
            "value": 15563
          }
        },
        "4770452fcc8d485c8ab8dae12c7937f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5c02c7993044fedb2c06a0637fe732a",
            "placeholder": "​",
            "style": "IPY_MODEL_d59184f34bc4413baf07e2f3efe26514",
            "value": " 15563/1946590 [00:21&lt;53:17, 603.98it/s]"
          }
        },
        "51e40810a55d4ff587b4ec5452a5776e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd1ab633a60f438e96549fb829066b30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1235c12bfcc4c3a9926e0216a533a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb79c74d916d42799dd7ac341fa69b0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b271af095877404dac02f67142e56ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5c02c7993044fedb2c06a0637fe732a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d59184f34bc4413baf07e2f3efe26514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}