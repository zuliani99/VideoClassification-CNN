{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1O6qJVSxsun"
      },
      "source": [
        "Installing *yt-dlp* to download the image frame from the YouTube videos and *pyarrow* "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-szf_YfTZ-",
        "outputId": "3e7ef5eb-3b3e-4611-f056-99f6e942b974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2023.3.4-py2.py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli\n",
            "  Downloading Brotli-1.0.9-cp39-cp39-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 KB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mutagen\n",
            "  Downloading mutagen-1.46.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets\n",
            "  Downloading websockets-11.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from yt-dlp) (2022.12.7)\n",
            "Collecting pycryptodomex\n",
            "  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.0.9 mutagen-1.46.0 pycryptodomex-3.17 websockets-11.0 yt-dlp-2023.3.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.9/dist-packages (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from pyarrow) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp\n",
        "!pip install pyarrow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjyedpkDx_p0"
      },
      "source": [
        "Importing all the useful packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tcf31PT9fYKS"
      },
      "outputs": [],
      "source": [
        "import yt_dlp\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import cv2\n",
        "import shutil\n",
        "import time\n",
        "import random\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import multiprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7Xtp0A6uTiN",
        "outputId": "c6702bad-03a5-456c-e6d1-047027adacb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Importing drive to download or update datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGoLZ4_ofeKZ"
      },
      "source": [
        "# Setting Up Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52DVZ517fan1"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "\n",
        "percentage_train_test = 10\n",
        "percentage_bag_shots = 1\n",
        "percentage_to_ignore = 10\n",
        "\n",
        "zip_f_name = f'VideoClassificationDataset_{percentage_train_test}_{percentage_bag_shots}_{percentage_to_ignore}'\n",
        "drive_dataset_folder = f'/content/drive/MyDrive/Datasets_VideoClassification/{zip_f_name}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlxRTLRo_TjR"
      },
      "source": [
        "##Download frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fxUL8gX6eJG"
      },
      "outputs": [],
      "source": [
        "dataset_path = Path('/content/VideoClassificationDataset')\n",
        "\n",
        "# Download the .zip file\n",
        "!wget --no-verbose https://github.com/gtoderici/sports-1m-dataset/archive/refs/heads/master.zip\n",
        "\n",
        "# Extract it \n",
        "!unzip -qq -o '/content/master.zip' \n",
        "!rm '/content/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xi72iBkwfdSO"
      },
      "outputs": [],
      "source": [
        "DATA = {'train_partition.txt': {},\n",
        "        'test_partition.txt': {}}\n",
        "\n",
        "LABELS = []\n",
        "\n",
        "train_dict = {}\n",
        "test_dict = {}\n",
        "\n",
        "path = '/content/sports-1m-dataset-master/original'\n",
        "\n",
        "# Populate the DATA dictionary by reading the train and test files\n",
        "for f in os.listdir(path):\n",
        "    with open(path + '/' + f) as f_txt:\n",
        "        lines = f_txt.readlines()\n",
        "        for line in lines:\n",
        "            splitted_line = line.split(' ')\n",
        "            label_indices = splitted_line[1].rstrip('\\n').split(',') \n",
        "            \n",
        "            DATA[f][splitted_line[0]] = list(map(int, label_indices))\n",
        "\n",
        "# Obtain the labels from the relative .txt file\n",
        "with open('/content/sports-1m-dataset-master/labels.txt') as f_labels:\n",
        "    LABELS = f_labels.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGQKz8h5fpQX",
        "outputId": "65beccc6-1f6e-4fbd-adca-5d8176a0280f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Train Test length:  914491 218667\n",
            "Sampled 10 % of Train & Test datasets, updated length:  91449 21866\n"
          ]
        }
      ],
      "source": [
        "TRAIN = DATA['train_partition.txt']\n",
        "TEST = DATA['test_partition.txt']\n",
        "print('Original Train Test length: ', len(TRAIN), len(TEST))\n",
        "\n",
        "# Sample a subset of percentage_train_test\n",
        "TRAIN = dict(random.sample(list(TRAIN.items()), (len(TRAIN)*percentage_train_test)//100))\n",
        "TEST = dict(random.sample(list(TEST.items()), (len(TEST)*percentage_train_test)//100))\n",
        "\n",
        "print(f'Sampled {percentage_train_test} % of Train & Test datasets, updated length: ', len(TRAIN), len(TEST))\n",
        "\n",
        "if not os.path.exists(dataset_path): os.makedirs(dataset_path)\n",
        "if not os.path.exists(f'{dataset_path}/train'): os.makedirs(f'{dataset_path}/train')\n",
        "if not os.path.exists(f'{dataset_path}/test'): os.makedirs(f'{dataset_path}/test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8vNk0MIf0S_"
      },
      "outputs": [],
      "source": [
        "def extract_frames(capture, directory, idx_bag, start_frame, end_frame):\n",
        "    '''\n",
        "    PORPOUSE: Extract frames from a given youtube video\n",
        "\n",
        "    TAKES:\n",
        "      - capture: cv2.VideoCapture(url) variable, with url being the link to the video\n",
        "      - directory: describe the saving directory\n",
        "      - idx_bag: indicate the index of the actual bag of shots\n",
        "      - start_frame: indicates the starting frame\n",
        "      - end_frame: indicates the ending frame\n",
        "\n",
        "    RETURNS:\n",
        "      True or False depending the presence of errors\n",
        "    '''\n",
        "\n",
        "    count = start_frame\n",
        "\n",
        "    # Set the next frame to download to 'count'\n",
        "    capture.set(cv2.CAP_PROP_POS_FRAMES, count)\n",
        "    os.makedirs(f'{directory}/bag_of_shots{str(idx_bag)}') # Create the relative directory\n",
        "\n",
        "    # Download the frame until we do not reach the end_frame\n",
        "    while count < end_frame:\n",
        "\n",
        "        ret, frame = capture.read() # Read the frame\n",
        "\n",
        "        if not ret: # In case there are errors, delete the aforementioned directory of bag of shots\n",
        "            shutil.rmtree(f'{directory}/bag_of_shots{str(idx_bag)}')\n",
        "            return False\n",
        "\n",
        "        # Save the readed frame in the directory of bag of shots resizing it to be 178x178\n",
        "        filename = f'{directory}/bag_of_shots{str(idx_bag)}/shot{str(count - start_frame)}.png'\n",
        "        write_res = cv2.imwrite(filename, cv2.resize(frame, (178, 178), interpolation = cv2.INTER_AREA))\n",
        "        if write_res:\n",
        "            count += 1    # If there is no error I increment the count\n",
        "        else:   # Otherwise I delete the all bag of shots returning Falses\n",
        "            shutil.rmtree(f'{directory}/bag_of_shots{str(idx_bag)}')\n",
        "            return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IfkSZDVf1zT"
      },
      "outputs": [],
      "source": [
        "def video_to_frames(video_url, label_id, directory, percentage_of_bags):\n",
        "    '''\n",
        "    PORPOUSE: Determine the amount of frame and bag of shots we want to downloads in each video, and perform the download of them\n",
        "\n",
        "    TAKES:\n",
        "      - video_url: complete url link to a specific YouTubbe video\n",
        "      - label_id: index label of the relative video\n",
        "      - idx_bag: indicate the index of the actual bag of shots\n",
        "      - percentage_of_bags: indicate the percentage of bags of shots that we want to download\n",
        "\n",
        "    RETURNS: \n",
        "      ret_dictionary: A dictionary containing as keys the path to a specific bag of shots and as value the list of labels for the \n",
        "                      relative bag, the last element of this list represent the number of frames we have downloaded\n",
        "    '''\n",
        "\n",
        "    url_id = video_url.split('=')[1]\n",
        "    path_until_url_id = f'{dataset_path}/{directory}/{url_id}'\n",
        "    \n",
        "    ret_dictionary = {}\n",
        "\n",
        "    try:   \n",
        "\n",
        "        # Setting up the dictionary options for yd-dlp\n",
        "        ydl_opts = {\n",
        "            'ignoreerrors': True,\n",
        "            'quiet': True,\n",
        "            'nowarnings': True,\n",
        "            'ignorenoformatserror': True,\n",
        "            'verbose':False,\n",
        "            'cookies': '/content/all_cookies.txt',\n",
        "            #https://stackoverflow.com/questions/63329412/how-can-i-solve-this-youtube-dl-429\n",
        "        }\n",
        "        ydl = yt_dlp.YoutubeDL(ydl_opts)\n",
        "        info_dict = ydl.extract_info(video_url, download=False) # Extracting the video infromation\n",
        "\n",
        "        if(info_dict is not None and  info_dict['fps'] >= 20):\n",
        "            # I must have a least 20 frames per seconds since I take half of second bag of shots for every video\n",
        "\n",
        "            formats = info_dict.get('formats', None)\n",
        "\n",
        "            # Excluding the initial and final choosen percentage of each video to avoid noise\n",
        "            video_length = info_dict['duration'] * info_dict['fps']\n",
        "\n",
        "            shots = info_dict['fps'] // 2\n",
        "\n",
        "            to_ignore = (video_length * percentage_to_ignore) // 100\n",
        "            new_len = video_length - (to_ignore * 2)\n",
        "            tot_stored_bags = ((new_len // shots) * percentage_of_bags) // 100   # ((total_possbile_bags // shots) * percentage_of_bags) // 100\n",
        "\n",
        "            if tot_stored_bags == 0: tot_stored_bags = 1 # I take at least a bag of shots\n",
        "\n",
        "            # Computing the skip rate between bags\n",
        "            skip_rate_between_bags = (new_len - (tot_stored_bags * shots)) // (tot_stored_bags-1) if tot_stored_bags > 1 else 0\n",
        "\n",
        "            chunks = [[to_ignore+(bag*(skip_rate_between_bags+shots)), to_ignore+(bag*(skip_rate_between_bags+shots))+shots] for bag in range(tot_stored_bags)]\n",
        "            # Sequence of [[start_frame, end_frame], [start_frame, end_frame], [start_frame, end_frame], ...]\n",
        "\n",
        "            format_id = {}\n",
        "            for f in formats: format_id[f['format_id']] = f\n",
        "\n",
        "            if '160' in list(format_id.keys()): # The downlaod is applyed only if the 144p format is available, 160 is the format_id for 144p\n",
        "                video = format_id['160']\n",
        "                url = video.get('url', None)\n",
        "                if(video.get('url', None) != video.get('manifest_url', None)): # We move forward only in case the url is different from the manifest_url\n",
        "\n",
        "                    if not os.path.exists(path_until_url_id): os.makedirs(path_until_url_id)\n",
        "                    \n",
        "                    capture = cv2.VideoCapture(url) # Initialize VideoCapture variable\n",
        "                    valid_chunks = 0\n",
        "\n",
        "                    for idx_bag, f in enumerate(chunks): # For each chunks\n",
        "\n",
        "                        # In case the download of the bag of shots succedeed\n",
        "                        if(extract_frames(capture, path_until_url_id, idx_bag, f[0], f[1])):\n",
        "                          \n",
        "                            l = np.array([label_id, shots])\n",
        "                            valid_chunks += 1\n",
        "\n",
        "                            ret_dictionary[f'{directory}/{url_id}/bag_of_shots{str(idx_bag)}'] = l.tolist() # Populate the new dictionary row\n",
        "\n",
        "                    # In case we do not have downloaded any chunks delete the directory and all its content\n",
        "                    if valid_chunks == 0: shutil.rmtree(path_until_url_id)\n",
        "\n",
        "                    capture.release()\n",
        "\n",
        "        return ret_dictionary\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # If an exception rised delete the directory with all its content and return ret_dictionary\n",
        "        if os.path.exists(path_until_url_id): shutil.rmtree(path_until_url_id)\n",
        "        return ret_dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8j8kijof3ix"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "count = 0\n",
        "\n",
        "# Perform a parallel download\n",
        "with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as pool:\n",
        "    with tqdm(total = len(TRAIN.items()), leave=False, desc='Downloading Train Dataset') as progress: # Setting up the progress bar\n",
        "        \n",
        "        futures = [] # Array of features\n",
        "\n",
        "        for url, label_id in TRAIN.items():\n",
        "            future = pool.submit(video_to_frames, url, label_id, 'train', percentage_bag_shots) # Assign the download\n",
        "            future.add_done_callback(lambda p: progress.update()) # Update the progrress bar\n",
        "            futures.append(future) # Append the feature to the features array\n",
        "\n",
        "        for future in futures:\n",
        "            if len(future.result()) > 0: # In case the result of the featue is not empty\n",
        "                train_dict.update(future.result()) # Append the result to the final dictionaty\n",
        "                count += 1\n",
        "\n",
        "\n",
        "print(f'--- Downloaded {count} videos frames in {(timedelta(seconds=(time.time() - start_time)))} (H:M:S:ms) ---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgZgTx5Vf72y"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "count = 0\n",
        "\n",
        "# Perform a parallel download\n",
        "with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as pool:\n",
        "    with tqdm(total = len(TEST.items()), leave=False, desc='Downloading Test Dataset') as progress: # Setting up the progress bar\n",
        "        \n",
        "        futures = [] # Array of features\n",
        "\n",
        "        for url, label_id in TEST.items():\n",
        "            future = pool.submit(video_to_frames, url, label_id, 'test', percentage_bag_shots) # Assign the download\n",
        "            future.add_done_callback(lambda p: progress.update()) # Update the progrress bar\n",
        "            futures.append(future) # Append the feature to the features array\n",
        "\n",
        "        for future in futures:\n",
        "            if len(future.result()) > 0: # In case the result of the featue is not empty\n",
        "                test_dict.update(future.result()) # Append the result to the final dictionaty\n",
        "                count += 1\n",
        "\n",
        "\n",
        "print(f'--- Downloaded {count} videos frames in {(timedelta(seconds=(time.time() - start_time)))} (H:M:S:ms) ---')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMEdQJ3O_lAw"
      },
      "source": [
        "## Save CSV and upload file Zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M65GudI_f9UT"
      },
      "outputs": [],
      "source": [
        "# Convert the dictionary to pandas DataFrame resetting the index\n",
        "train_df = pd.DataFrame.from_dict(train_dict, orient='index', dtype=int).reset_index(level=0)\n",
        "train_df = train_df.rename(columns={train_df.columns[-1]: 'shots'}) # Rename the last column as 'shots'\n",
        "train_df.columns = train_df.columns.astype(str) # Setting the column type as string\n",
        "train_df.to_parquet(f'{dataset_path}/train.parquet', index=True) # Generation the parquet file\n",
        "\n",
        "# Convert the dictionary to pandas DataFrame resetting the index\n",
        "test_df = pd.DataFrame.from_dict(test_dict, orient='index', dtype=int).reset_index(level=0)\n",
        "test_df = test_df.rename(columns={test_df.columns[-1]: 'shots'}) # Rename the last column as 'shots'\n",
        "test_df.columns = test_df.columns.astype(str) # Setting the column type as string\n",
        "test_df.to_parquet(f'{dataset_path}/test.parquet', index=True) # Generation the parquet file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLp5bATpzwDV"
      },
      "outputs": [],
      "source": [
        "shutil.make_archive('train', 'zip', f'{dataset_path}/train') # Create a train.zip file\n",
        "shutil.make_archive('test', 'zip', f'{dataset_path}/test') # Create a test.zip file\n",
        "os.mkdir(drive_dataset_folder) # Create remote drive folder\n",
        "\n",
        "# Coping the .zip, .parquet and .txt file into the remote directory\n",
        "shutil.copy('/content/sports-1m-dataset-master/labels.txt', drive_dataset_folder)\n",
        "shutil.copy(f'{dataset_path}/train.parquet', drive_dataset_folder)\n",
        "shutil.copy(f'{dataset_path}/test.parquet', drive_dataset_folder)\n",
        "shutil.copy(f'{dataset_path}/train.zip', drive_dataset_folder)\n",
        "shutil.copy(f'{dataset_path}/test.zip', drive_dataset_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMTDALJo_tYE"
      },
      "source": [
        "## Downloadload Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f58b3eb352994a65a2bbf0bce2288b38",
            "ce4d2a7fccf04446af789387bc4b4c9e",
            "e68efad9e7e245feae3b810d59efc489",
            "b22b3c1ef1974219adbac12f6ac18b3d",
            "c3bbc4dd900b483b81078d55b2b48c07",
            "12c2d3c68c63440e96cab644622c8ac6",
            "7a96039ea52944479933d5b3906299f6",
            "bbc201a49d38481dae6f5b86372bead3",
            "a78dda45698841a28bcb4babd9b522df",
            "b2fbc3fe3e19452588443c626bd5753a",
            "2e2a9235ea0a467fbd26b827363cef1f"
          ]
        },
        "id": "Yr0R9hdz1Ih1",
        "outputId": "1ecd82c6-c0ae-4c20-fe3f-5a91f5dabe54"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unzipping train.zip:   0%|          | 0/1592774 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f58b3eb352994a65a2bbf0bce2288b38"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# With ZipFile I also download the .zip and at the same time unzipping it so I need the double capacity\n",
        "\n",
        "dataset_path = Path('/content/VideoClassificationDataset')\n",
        "\n",
        "# Unzip the train and test zip files in Colab Storage\n",
        "for z in ['train', 'test']:\n",
        "    with zipfile.ZipFile(f'{drive_dataset_folder}/{z}.zip', 'r') as zip_ref:\n",
        "        for file in tqdm(iterable=zip_ref.namelist(), total=len(zip_ref.namelist()), desc=f'Unzipping {z}.zip'):\n",
        "            zip_ref.extract(member=file, path=dataset_path)\n",
        "\n",
        "# Copy the train and test .parquet and labels.txt in Colab Storage\n",
        "shutil.copy(f'{drive_dataset_folder}/train.parquet', dataset_path)\n",
        "shutil.copy(f'{drive_dataset_folder}/test.parquet', dataset_path)\n",
        "shutil.copy(f'{drive_dataset_folder}/labels.txt', dataset_path)\n",
        "\n",
        "print('All done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlysu66Ww42s"
      },
      "source": [
        "### Read labels.txt, train.parquet and test.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK-E6raIjzEp"
      },
      "outputs": [],
      "source": [
        "LABELS = []\n",
        "\n",
        "train_dict = {}\n",
        "test_dict = {}\n",
        "\n",
        "# Obtain the list of labels\n",
        "with open(f'{dataset_path}/labels.txt') as f_labels:\n",
        "    LABELS = f_labels.read().splitlines()\n",
        "\n",
        "train_df = pd.read_parquet(f'{dataset_path}/train.parquet') # Read the parquet train dataset\n",
        "test_df = pd.read_parquet(f'{dataset_path}/test.parquet') # Read the parquet test dataset\n",
        "\n",
        "\n",
        "for string, df, dic in zip(('train', 'test'), (train_df, test_df), (train_dict, test_dict)):\n",
        "    for k,v in tqdm(df.T.items(), total=len(list(df.T.items())), desc=f'Populating {string} dictionary'):\n",
        "        values = v.to_numpy()\n",
        "        dic[values[0]] = list(values[1:]) # Populate the dictionary\n",
        "\n",
        "\n",
        "print(f'\\nSampled {percentage_train_test} % of Train & Test datasets')\n",
        "print(f'Number of train bags of shots: {len(train_df)}')\n",
        "print(f'Number of test bags of shots: {len(test_dict)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6FtV3EUFkAI"
      },
      "source": [
        "### Data Consistency Checking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXPkyNgc_IvI"
      },
      "outputs": [],
      "source": [
        "# Check the presence of empty or inconsistent directories\n",
        "\n",
        "print(train_df[train_df['shots'] < 5][['index', 'shots']])\n",
        "print(test_df[test_df['shots'] < 5][['index', 'shots']])\n",
        "\n",
        "for idx, row in train_df[train_df['shots'] < 5].iterrows():\n",
        "    print(row['index'], os.listdir(f\"{dataset_path}/{row['index']}\"))\n",
        "\n",
        "for idx, row in test_df[test_df['shots'] < 5].iterrows():\n",
        "    print(row['index'], os.listdir(f\"{dataset_path}/{row['index']}\"))\n",
        "\n",
        "to_del_train = []\n",
        "for idx, row in train_df.iterrows():\n",
        "    for shot_id in range(row['shots']):\n",
        "        if f'shot{shot_id}.png' not in os.listdir(f\"{dataset_path}/{row['index']}\"):\n",
        "            print(row['index'], os.listdir(f\"{dataset_path}/{row['index']}\"))\n",
        "            to_del_train.append(idx)\n",
        "            break\n",
        "\n",
        "to_del_test = []\n",
        "for idx, row in test_df.iterrows():\n",
        "    for shot_id in range(row['shots']):\n",
        "        if f'shot{shot_id}.png' not in os.listdir(f\"{dataset_path}/{row['index']}\"):\n",
        "            print(row['index'], os.listdir(f\"{dataset_path}/{row['index']}\"))\n",
        "            to_del_test.append(idx)\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxaUROjI2ZKQ"
      },
      "outputs": [],
      "source": [
        "# Throw away those directory with correspondant row of dictionary and dataframe\n",
        "\n",
        "to_delete_train = train_df[train_df['shots'] < 5]\n",
        "to_delete_train = pd.concat([to_delete_train, train_df.iloc[to_del_train]], axis=0)\n",
        "train_df.drop(to_delete_train.index, axis=0, inplace=True)\n",
        "train_df.reset_index()\n",
        "\n",
        "for idx, to_d in to_delete_train.iterrows():\n",
        "    del train_dict[to_d['index']]\n",
        "    shutil.rmtree(f\"{dataset_path}/{to_d['index']}\")\n",
        "\n",
        "\n",
        "to_delete_test = test_df[test_df['shots'] < 5]\n",
        "to_delete_test = pd.concat([to_delete_test, test_df.iloc[to_del_test]], axis=0)\n",
        "test_df.drop(to_delete_test.index, axis=0, inplace=True)\n",
        "test_df.reset_index()\n",
        "\n",
        "for idx, to_d in to_delete_test.iterrows():\n",
        "    del test_dict[to_d['index']]\n",
        "    shutil.rmtree(f\"{dataset_path}/{to_d['index']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC_q1x0-fjLN"
      },
      "source": [
        "# Neural Networks Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnXsGuN8Ea0u"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from timeit import default_timer as timer \n",
        "import copy\n",
        "\n",
        "from google.colab import widgets\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "print(f'PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDCWzDD4ExEk"
      },
      "outputs": [],
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "device # Setup the device type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_mo3Ku6Gsud"
      },
      "outputs": [],
      "source": [
        "# Get the train and test path\n",
        "train_dir = dataset_path / 'train'\n",
        "test_dir = dataset_path / 'test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0POrdf00zQI7"
      },
      "source": [
        "## Initial analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF2sRptTfwS_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE: Obtaining the label name and index of a specific image\n",
        "\n",
        "TAKES:\n",
        "  - dictionary: is the dictionary from where we have to extract the ids\n",
        "  - path: describe the path of the specific bag of shots\n",
        "\n",
        "RETURNS: Pairs of label index and name\n",
        "'''\n",
        "\n",
        "def get_label_id_name(dictionary, path):\n",
        "    id = np.where(np.asarray(dictionary[path]) == 1)[0]\n",
        "    return id, LABELS[id] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ud9qlogG654"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE: Plot a random dataset image with its informations\n",
        "\n",
        "TAKES:\n",
        "  - dictionary: is the dictionary from where we have to extract the ids\n",
        "  - image_path_list: describe the path where we have to sample a image\n",
        "\n",
        "RETURNS: Pillow image\n",
        "'''\n",
        "\n",
        "def plot_random_image(dictionary, image_path_list):\n",
        "    random_image_path = random.choice(image_path_list) # Sample a image\n",
        "\n",
        "    path = os.path.relpath(random_image_path.parent, dataset_path) # Get the relative path\n",
        "    \n",
        "    image_class_name, image_class_id = get_label_id_name(dictionary, path)\n",
        "    img = Image.open(random_image_path) # Open the image\n",
        "\n",
        "    print(f'Random image path: {random_image_path}')\n",
        "    print(f'Image classes id: {image_class_id}')\n",
        "    print(f'Image classes name: {image_class_name}')\n",
        "    print(f'Image height: {img.height}') \n",
        "    print(f'Image width: {img.width}')\n",
        "\n",
        "    return img\n",
        "\n",
        "plot_random_image(train_dict, list(train_dir.glob('*/*/*.png')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdHloqARHVoq"
      },
      "source": [
        "Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYxaxNDTHXVi"
      },
      "outputs": [],
      "source": [
        "# Setting up the train and test transofms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.4588,0.4588,0.4588], std=[0.4588,0.4588,0.4588])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=[0.4588,0.4588,0.4588], std=[0.4588,0.4588,0.4588])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LT7QuxhzKMV"
      },
      "source": [
        "## From Dataset to Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_qzUtoXJGY9"
      },
      "source": [
        "Dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRzgmcBbBPYt"
      },
      "outputs": [],
      "source": [
        "# VideoDataset object \n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, df, transform = None, t = 'single'):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.t = t\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.transform is None: self.transform = transforms.ToTensor()\n",
        "        images_path = self.df.iloc[index, 0]\n",
        "\n",
        "        shots = self.df.iloc[index, -1] # Get the number of frames of a bag of shots\n",
        "        # Each bag is half second frames\n",
        "\n",
        "        if self.t == 'single':\n",
        "            # I want only the central frame\n",
        "            images = self.transform(Image.open(f'{dataset_path}/{images_path}/shot{shots//2}.png'))\n",
        "\n",
        "        if self.t == 'early':\n",
        "            # I want the 5 middle frames\n",
        "            images = np.array([self.transform(Image.open(f'{dataset_path}/{images_path}/shot{idx}.png')).numpy() for idx in range((shots//2)-2,(shots//2)+3)])\n",
        "        \n",
        "        if self.t == 'late':\n",
        "            # I want the first and last frames\n",
        "            images = np.array([self.transform(Image.open(f'{dataset_path}/{images_path}/shot0.png')).numpy(), self.transform(Image.open(f'{dataset_path}/{images_path}/shot{shots-1}.png')).numpy()])\n",
        "        \n",
        "        if self.t == 'slow':\n",
        "            # I want the 10 middle frames\n",
        "            if shots%10 == 0:\n",
        "                images = np.array([self.transform(Image.open(f'{dataset_path}/{images_path}/shot{idx}.png')).numpy() for idx in range((shots//2) - 5, (shots//2) + 5)])\n",
        "            else:\n",
        "                images = np.array([self.transform(Image.open(f'{dataset_path}/{images_path}/shot{idx}.png')).numpy() for idx in range((shots%10) - (shots%10)//2, shots-(shots%10)//2)])\n",
        "\n",
        "        y_label = torch.from_numpy(self.df.iloc[index, 1:-1].to_numpy().astype(float)) # Get the labels\n",
        "\n",
        "        if self.t != 'single': images = torch.from_numpy(images) # Convert the numpy image to tensor in case I want the central frame\n",
        "\n",
        "        return images, y_label, images_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adtfogD8K0xz"
      },
      "source": [
        "Split the training set into train and validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fki8FMP_K0gZ"
      },
      "outputs": [],
      "source": [
        "def spit_train(train_data, perc_val_size):\n",
        "    '''\n",
        "    PORPOUSE: Splot the dataset in train and validation set\n",
        "\n",
        "    TAKES:\n",
        "      - train_data: dataset to split\n",
        "      - perc_val_size: percentage of split\n",
        "\n",
        "    RETURNS: train dataset and validation dataset\n",
        "    '''\n",
        "    \n",
        "    train_size = len(train_data)\n",
        "    val_size = int((train_size * perc_val_size) // 100)\n",
        "    train_size -= val_size\n",
        "\n",
        "    return random_split(train_data, [int(train_size), int(val_size)]) #train_data, val_data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9xtNId8vjFa"
      },
      "source": [
        "Dataset definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX_hl5ESKuj7"
      },
      "outputs": [],
      "source": [
        "# Let's set the validations set to the 20% of the train dataset\n",
        "\n",
        "# Dataset for Single Frame\n",
        "train_data_single, val_data_single = spit_train(VideoDataset(df=train_df, transform=train_transform, t='single'), 20)\n",
        "test_data_single = VideoDataset(df=test_df, transform=test_transform, t='single')\n",
        "\n",
        "# Dataset for Multi Frame - Early Fusion\n",
        "train_data_early, val_data_early = spit_train(VideoDataset(df=train_df, transform=train_transform, t='early'), 20)\n",
        "test_data_early = VideoDataset(df=test_df, transform=test_transform, t='early')\n",
        "\n",
        "# Dataset for Multi Frame - Late Fusion\n",
        "train_data_late, val_data_late = spit_train(VideoDataset(df=train_df, transform=train_transform, t='late'), 20)\n",
        "test_data_late = VideoDataset(df=test_df, transform=test_transform, t='late')\n",
        "\n",
        "# Dataset for Multi Frame - Slow Fusion\n",
        "train_data_slow, val_data_slow = spit_train(VideoDataset(df=train_df, transform=train_transform, t='slow'), 20)\n",
        "test_data_slow = VideoDataset(df=test_df, transform=test_transform, t='slow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wymzpd8yfBAU"
      },
      "source": [
        "### Random Image from the train and test dataloader for Single Frame model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKsG07QC_y4k"
      },
      "outputs": [],
      "source": [
        "def random_image_from_dataloader(dt, dic):\n",
        "    '''\n",
        "    PORPOUSE: Plot a random image from a given dataset\n",
        "\n",
        "    TAKES:\n",
        "      - dt: dataset where sample a image\n",
        "      - dic: dictionary to which the image belongs\n",
        "\n",
        "    RETURNS: train dataset and validation dataset\n",
        "    '''\n",
        "    \n",
        "    rnd = random.choice(dt)\n",
        "    img, label, path = rnd\n",
        "    label_id, label_name = get_label_id_name(dic, path)\n",
        "\n",
        "    print(f'Image tensor:\\n{img}')\n",
        "    print(f'Image shape: {img.shape}')\n",
        "    print(f'Image datatype: {img.dtype}')\n",
        "    print(f'Image label ids: {label_id}')\n",
        "    print(f'Image label names: {label_name}')\n",
        "    print(f'Label datatype: {type(label)}')\n",
        "    plt.imshow(img.permute(1, 2, 0).numpy())\n",
        "    plt.axis(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGF5v86820RF"
      },
      "outputs": [],
      "source": [
        "random_image_from_dataloader(train_data_single, train_dict) # Random image from the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwqji2FGb_El"
      },
      "outputs": [],
      "source": [
        "random_image_from_dataloader(test_data_single, test_dict) # Random image from the test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A9MPVAYM4hi"
      },
      "source": [
        "\n",
        "### Turn loaded images into DataLoader's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp3ZwwtosfCP"
      },
      "outputs": [],
      "source": [
        "# Turn train and test satasets into DataLoaders\n",
        "BATCH_SIZE = 32 # <---------------------------- TODO: tune this paramenter\n",
        "NUM_WORKERS = os.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SomP75Zsd0U"
      },
      "outputs": [],
      "source": [
        "def generate_dataloaders(train_data, val_data, test_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
        "    '''\n",
        "    PORPOUSE: generate the train vaiadtion and test dataloaders\n",
        "\n",
        "    TAKES:\n",
        "      - train_data: train dataset\n",
        "      - val_data: validation dataset\n",
        "      - test_data: test dataset\n",
        "      - batch_size: specify how many bags of shots a bach must have\n",
        "      - num_workers: specify how many workers will work on the dataloaders creation \n",
        "\n",
        "    RETURNS: train, val and test dataloaders\n",
        "    '''\n",
        "    \n",
        "    train_dl = DataLoader(dataset = train_data, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle = True)\n",
        "    val_dl = DataLoader(dataset = val_data, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle = True)\n",
        "    test_dl = DataLoader(dataset = test_data, batch_size = BATCH_SIZE, num_workers = NUM_WORKERS, shuffle = False)\n",
        "\n",
        "    return train_dl, val_dl, test_dl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfHVNSmHuZsb"
      },
      "source": [
        "## Models definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk_25zkeucFh"
      },
      "outputs": [],
      "source": [
        "# Defining the main custom Alexnet that ereditate the fucntion of nn.Module\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, in_channels, stream_type=None, t_frames=[1,1,1]):\n",
        "        # stream_type: none, \"context\", \"fovea\"\n",
        "        # None is used for considering the cnn with spartial fusion informations\n",
        "        super().__init__()\n",
        "\n",
        "        # input size nomultiresulution : (b x 3 x 170 x 170)\n",
        "        # input size multiresulution : (b x 3 x 178 x 178) take into account the video classifier -> which becomes 89 x 89 in both streams\n",
        "\n",
        "        self.stream_type = stream_type\n",
        "        self.fovea = transforms.Compose([transforms.CenterCrop((89, 89))])\n",
        "        self.context = transforms.Compose([transforms.Resize((89, 89))])\n",
        "        self.transform = transforms.Compose([transforms.Resize((170, 170))])\n",
        "        self.t_frames = t_frames\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels*self.t_frames[0], 96, kernel_size=11, stride=3, padding=2),  \n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 96 x 170 x 170) -> (b x 96 x 55 x 55)\n",
        "            # MULTIRES -> in: (b x 96 x 89 x 89) -> (b x 96 x 28 x 28)\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.5, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 96 x 55 x 55) -> (b x 96 x 28 x 28)\n",
        "            # MULTIRES -> in: (b x 96 x 28 x 28) -> (b x 96 x 14 x 14)\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(96*self.t_frames[1], 256, kernel_size=5, stride=1, padding=2), \n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 96 x 28 x 28) -> (b x 256 x 28 x 28)\n",
        "            # MULTIRES -> in: (b x 96 x 14 x 14) -> (b x 256 x 14 x 14)\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.5, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 256 x 28 x 28) -> (b x 256 x 14 x 14)\n",
        "            # MULTIRES -> in: (b x 256 x 14 x 14) -> (b x 256 x 7 x 7)\n",
        "        )\n",
        "        \n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256*self.t_frames[2], 384, kernel_size=3, stride=1, padding=1), \n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 256 x 14 x 14) -> (b x 384 x 14 x 14)\n",
        "            # MULTIRES -> in: (b x 256 x 7 x 7) -> (b x 384 x 7 x 7)\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 384 x 14 x 14) -> (b x 384 x 14 x 14)\n",
        "            # MULTIRES -> in: (b x 384 x 7 x 7) -> (b x 384 x 7 x 7)\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            # CORRECT\n",
        "            # NO-MULTIRES -> in: (b x 384 x 14 x 14) -> (b x 256 x 14 x 14)\n",
        "            # MULTIRES -> in: (b x 384 x 7 x 7) -> (b x 256 x 7 x 7)\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.MaxPool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # CORRECT\n",
        "        # NO-MULTIRES -> in: (b x 256 x 14 x 14) -> (b x 256 x 7 x 7)\n",
        "\n",
        "        self.init_bias()  # initialize bias -> CHECK IF IT MAKE SENSE\n",
        "\n",
        "    def init_bias(self):\n",
        "        for block in [self.conv1, self.conv2, self.conv3]:\n",
        "            for layer in block:\n",
        "                if isinstance(layer, nn.Conv2d):\n",
        "                    nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "                    nn.init.constant_(layer.bias, 0)\n",
        "        # original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers\n",
        "        nn.init.constant_(self.conv2[0].bias, 1)\n",
        "        nn.init.constant_(self.conv3[2].bias, 1)\n",
        "        nn.init.constant_(self.conv3[4].bias, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.stream_type != None:\n",
        "            x = self.fovea(x) if self.stream_type == 'fovea' else self.context(x)\n",
        "            return self.conv3(self.conv2(self.conv1(x)))\n",
        "        else: \n",
        "            x = self.transform(x)\n",
        "            return self.MaxPool(self.conv3(self.conv2(self.conv1(x))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRVs6r74wQDu"
      },
      "source": [
        "### Single frame CNNs:\n",
        "1. Single-Fame (custom AlexNet)\n",
        "2. Single-Fame Fovea Only\n",
        "3. Single-Fame Context Only\n",
        "4. Single-Fame + Multires\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJYGNCZWvvCp"
      },
      "outputs": [],
      "source": [
        "# The single frame CNN, includes the three CNNs from the above list\n",
        "\n",
        "class SingleResCNN(nn.Module):\n",
        "    def __init__(self, CNN, num_classes):\n",
        "        super(SingleResCNN, self).__init__()\n",
        "        self.CNN = CNN\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=(256 * 7 * 7), out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.CNN(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obJWjr0qvtYd"
      },
      "outputs": [],
      "source": [
        "# Defining the single frame multiresolution CNN \n",
        "\n",
        "class MultiResCNN(nn.Module):\n",
        "    def __init__(self, AlexNet_fovea, AlexNet_context, num_classes):\n",
        "        super(MultiResCNN, self).__init__()\n",
        "        self.AlexNet_fovea = AlexNet_fovea\n",
        "        self.AlexNet_context = AlexNet_context\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=(512 * 7 * 7), out_features=4096), # 512 since it takes the double of the infrormations\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x1 = self.AlexNet_fovea(x.clone()) # Takes the fovea strea\n",
        "        x2 = self.AlexNet_context(x.clone()) # Takes the contxt stream\n",
        "        x = torch.cat((x1, x2), dim=1)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZGMVBhfbwb9"
      },
      "source": [
        "### Multi Frames CNNs\n",
        "1. Late Fusion\n",
        "2. Early Fusion\n",
        "3. Slow Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lj0WeGNvwmh"
      },
      "outputs": [],
      "source": [
        "# Late Fusion Model\n",
        "\n",
        "class LateFusionCNN(nn.Module):\n",
        "    def __init__(self, AlexNet_1, AlexNet_2, num_classes):\n",
        "        super(LateFusionCNN, self).__init__()\n",
        "        self.AlexNet_1 = AlexNet_1 # Two Separates AlexNet\n",
        "        self.AlexNet_2 = AlexNet_2\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=(512 * 7 * 7), out_features=4096),  # 512 since it takes the double of the infrormations\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        initial = torch.empty(x.shape[0], 3, 178, 178).to(device)\n",
        "        final = torch.empty(x.shape[0], 3, 178, 178).to(device)\n",
        "        \n",
        "        for i in range(x.shape[0]):\n",
        "            initial[i] = x[i][0]\n",
        "            final[i] = x[i][1]\n",
        "\n",
        "        initial = self.AlexNet_1(initial)\n",
        "        final = self.AlexNet_2(final)\n",
        "        x = torch.cat((initial, final), dim=1)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzk4JoNYvyCW"
      },
      "outputs": [],
      "source": [
        "# Early Fusion Model\n",
        "\n",
        "class EarlyFusionCNN(nn.Module):\n",
        "    def __init__(self, AlexNet, num_classes):\n",
        "        super(EarlyFusionCNN, self).__init__()\n",
        "        self.AlexNet = AlexNet\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=(256 * 7 * 7), out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.reshape(x.shape[0], x.shape[1]*3, 178, 178)\n",
        "        x = self.AlexNet(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx_BjklmvzeU"
      },
      "outputs": [],
      "source": [
        "# Sow Fusion Model\n",
        "\n",
        "class SlowFusionCNN(nn.Module):\n",
        "    def __init__(self, AlexNet, num_classes):\n",
        "        super(SlowFusionCNN, self).__init__()\n",
        "        self.AlexNet = AlexNet\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=(256 * 7 * 7), out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        bag = torch.empty(4,x.shape[0],3*self.AlexNet.t_frames[0],178,178).to(device)\n",
        "        \n",
        "        for idx in range(bag.shape[0]): # 0 - 4\n",
        "            for i in range(x.shape[0]): # 0 - batch_size\n",
        "                bag[idx][i] = x[i][(idx) : (idx+4)].reshape(3*self.AlexNet.t_frames[0],178,178)\n",
        "\n",
        "        rconv2_2 = torch.cat((self.AlexNet.conv2( # Second layer\n",
        "            torch.cat((self.AlexNet.conv1(bag[0]), self.AlexNet.conv1(bag[1])), dim=1) # First layer\n",
        "        ), self.AlexNet.conv2( # Second layer\n",
        "            torch.cat((self.AlexNet.conv1(bag[2]), self.AlexNet.conv1(bag[3])), dim=1) # First layer\n",
        "        )), dim=1)\n",
        "\n",
        "        x = self.AlexNet.MaxPool(self.AlexNet.conv3(rconv2_2)) # Third layer\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        return self.classifier(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPU84FGDFw5G"
      },
      "source": [
        "### Train and Evaluate Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L43iesXbv04o"
      },
      "outputs": [],
      "source": [
        "# CNN Architecture to perform the Train and Evaluation steps saving the results\n",
        "\n",
        "class CNN_Architecture():\n",
        "\n",
        "    def __init__(self, model: torch.nn.Module, train_dataloader: torch.utils.data.DataLoader, \n",
        "        val_dataloader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer,\n",
        "        loss_fn: torch.nn.Module, score_fn, scheduler: torch.optim.Optimizer, device: torch.device,\n",
        "        save_check = False, load_check_train = False, load_check_evaluate = False):\n",
        "\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.loss_fn = loss_fn\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.score_fn = score_fn\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        self.save_check = save_check\n",
        "        self.load_check_train = load_check_train\n",
        "        self.load_check_evaluate = load_check_evaluate\n",
        "        if self.model.__class__.__name__ == 'SingleResCNN':\n",
        "            self.model_name = f'{self.model.__class__.__name__}-Stream_Type_{self.model.CNN.stream_type}'\n",
        "        else: \n",
        "            self.model_name = self.model.__class__.__name__\n",
        "        self.best_checkpoint_filename = f'/content/drive/MyDrive/checkpoints/best_checkpoints/{self.model_name}_checkpoint.pth.tar'\n",
        "        self.last_checkpoint_filename = f'/content/drive/MyDrive/checkpoints/last_checkpoints/{self.model_name}_checkpoint.pth.tar'\n",
        "\n",
        "\n",
        "\n",
        "    def __save_checkpoint(self, results, checkpoint_filename, best_train_loss = None, best_train_accuracy = None):\n",
        "        '''\n",
        "        PORPOUSE: save the a checkpoint model\n",
        "\n",
        "        TAKES:\n",
        "          - results: dictionary containing all the results up that epoch\n",
        "          - checkpoint_filename: google drive path where to save the checkpoint \n",
        "          - best_train_loss: best train loss to save\n",
        "          - best_train_accuracy: best train accuracy score to save\n",
        "\n",
        "        RETURNS: None\n",
        "        '''\n",
        "\n",
        "        print(f'=> Saving Checkpoint to {checkpoint_filename.split(\"/\")[5]}')\n",
        "        checkpoint = {'state_dict': self.model.state_dict(), 'optimizer': self.optimizer.state_dict(), 'scheduler': self.scheduler.state_dict(),\n",
        "                      'results': results, 'best_train_loss': best_train_loss, 'best_train_accuracy': best_train_accuracy}\n",
        "        torch.save(checkpoint, checkpoint_filename)\n",
        "        print(' DONE\\n')\n",
        "\n",
        "\n",
        "    \n",
        "    def __load_best_checkpoint(self):\n",
        "        '''\n",
        "        PORPOUSE: Load the best chekpoint model\n",
        "\n",
        "        RETURNS: None\n",
        "        '''\n",
        "\n",
        "        print('=> Loading Best Checkpoint')\n",
        "        checkpoint = torch.load(self.best_checkpoint_filename)\n",
        "        self.model.load_state_dict(checkpoint['state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "        print(' DONE\\n')\n",
        "\n",
        "\n",
        "    def __load_last_checkpoint(self):\n",
        "        '''\n",
        "        PORPOUSE: Load the last chekpoint model\n",
        "\n",
        "        RETURNS: results fit history\n",
        "        '''\n",
        "\n",
        "        print('=> Loading Last Checkpoint')\n",
        "        checkpoint = torch.load(self.best_checkpoint_filename)\n",
        "        self.model.load_state_dict(checkpoint['state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "        print(' DONE\\n')\n",
        "        return checkpoint['results'], checkpoint['best_train_loss'], checkpoint['best_train_accuracy']\n",
        "\n",
        "    \n",
        "    def evaluate(self, val_dataloader: torch.utils.data.DataLoader, epoch = 0, epochs = 1):\n",
        "        '''\n",
        "        PORPOUSE: Perform the model evaluation / testing\n",
        "\n",
        "        TAKES:\n",
        "          - val_dataloader: checkpoint to load\n",
        "          - epoch / epochs\n",
        "\n",
        "        RETURNS: dictionary containing the model name, loss and socore\n",
        "        '''\n",
        "\n",
        "        val_loss, val_accuracy = 0, 0\n",
        "\n",
        "        if self.load_check_evaluate: self.__load_best_checkpoint() # If the flag is true I load the best checkpoint\n",
        "\n",
        "        self.model.eval() # Evaluation phase\n",
        "\n",
        "        pbar = tqdm(enumerate(val_dataloader), total = len(val_dataloader), leave=False) # Initialize the progress bar\n",
        "\n",
        "        with torch.inference_mode(): # Allow inference mode\n",
        "            for batch_idx, (images, label, _) in pbar:\n",
        "                images, label = images.to(self.device), label.to(self.device) # Move the images and labels into the device\n",
        "\n",
        "                outputs = self.model(images) # Get the model output\n",
        "\n",
        "                loss = self.loss_fn(outputs, label) # Get the loss\n",
        "\n",
        "                accuracy = self.score_fn(outputs, label).item() # Perform the score\n",
        "\n",
        "                # Increment the statistics\n",
        "                val_loss += loss.item()\n",
        "                val_accuracy += accuracy\n",
        "\n",
        "                # Update the progress bar\n",
        "                if epoch > 0: pbar.set_description(f'{self.model_name} EVALUATION Epoch [{epoch + 1} / {epochs}]')\n",
        "                else: pbar.set_description(f'{self.model_name} TESTING')\n",
        "                pbar.set_postfix(loss = loss.item(), accuracy = accuracy)\n",
        "              \n",
        "            val_loss /= len(val_dataloader) # Calculate the final loss\n",
        "            val_accuracy /= len(val_dataloader) # Calculate the final score\n",
        "\n",
        "\n",
        "        return { 'model_name': self.model_name,\n",
        "                'model_loss': val_loss,\n",
        "                'model_accuracy': val_accuracy }\n",
        "\n",
        "\n",
        "    \n",
        "    def fit(self, epochs: int):\n",
        "        '''\n",
        "        PORPOUSE: Perform the model traing\n",
        "\n",
        "        TAKES:\n",
        "          - epochs: number of times that our model will see the data\n",
        "\n",
        "        RETURNS: dictionary of results containing model name and history of results for each epoch\n",
        "        '''\n",
        "\n",
        "        results = { 'train_loss': [], 'train_v': [], 'val_loss': [], 'val_accuracy': [] }\n",
        "        best_train_loss, best_train_v = float('inf'), float('-inf')\n",
        "\n",
        "        if self.load_check_train:\n",
        "            # If the flag is true I load the last checkpoint, importing the previous results history whereas the best loss and accuracy score\n",
        "            results, best_train_loss, best_train_v = self.__load_last_checkpoint() \n",
        "\n",
        "        for epoch in range(len(results['train_loss']), epochs):\n",
        "            train_loss, train_accuracy = 0, 0\n",
        "\n",
        "            pbar = tqdm(enumerate(self.train_dataloader), total = len(self.train_dataloader), leave=False) # Initialize the progress bar\n",
        "            \n",
        "            for batch_idx, (images, label, _) in pbar:\n",
        "\n",
        "                self.model.train() # Training phase\n",
        "\n",
        "                # zero_grad -> backword -> step\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                images, label = images.to(self.device), label.to(self.device) # Move the images and labels into the device\n",
        "                \n",
        "                outputs = self.model(images) # Get the model output\n",
        "\n",
        "                loss = self.loss_fn(outputs, label) # Get the loss\n",
        "\n",
        "                loss.backward() # Backword step\n",
        "                self.optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                accuracy = self.score_fn(outputs, label).item() # Perform the score\n",
        "\n",
        "                train_accuracy += accuracy\n",
        "\n",
        "\n",
        "                # Update the progress bar\n",
        "                pbar.set_description(f'{self.model_name} TRAIN Epoch [{epoch + 1} / {epochs}]')\n",
        "                pbar.set_postfix(loss = loss.item(), accuracy = accuracy)\n",
        "\n",
        "\n",
        "            train_loss /= len(self.train_dataloader) # Calculate the final loss\n",
        "            train_accuracy /= len(self.train_dataloader) # Calculate the final score\n",
        "\n",
        "\n",
        "            self.scheduler.step(train_loss)\n",
        "            \n",
        "\n",
        "            # Validation phase\n",
        "            model_name, val_loss, val_accuracy = (self.evaluate(self.val_dataloader, epoch, epochs)).values()\n",
        "\n",
        "            # Append the results of the current epoch\n",
        "            results['train_loss'].append(train_loss)\n",
        "            results['train_accuracy'].append(train_accuracy)\n",
        "            results['val_loss'].append(val_loss)\n",
        "            results['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "            print('Epoch [{}], train_loss: {:.6f}, train_accuracy: {:.6f}, val_loss: {:.6f}, val_accuracy: {:.6f} \\n'.format(\n",
        "                  epoch + 1, train_loss, train_accuracy, val_loss, val_accuracy))\n",
        "            \n",
        "\n",
        "            if(self.save_check): \n",
        "                self.__save_checkpoint(results, self.last_checkpoint_filename, train_loss, train_accuracy) # Save a checkpoint \n",
        "                if(train_loss < best_train_loss and train_accuracy > best_train_accuracy): # Save a checkpoint if the actual results are the best one\n",
        "                    self.__save_checkpoint(results, self.best_checkpoint_filename)\n",
        "                    best_train_loss,best_train_accuracy  = train_loss, train_accuracy\n",
        "\n",
        "        return {'model_name': self.model_name, 'results': results}\n",
        "\n",
        "    \n",
        "    \n",
        "    def evaluate_and_plot_image(self, images_tensor, class_names, transform=None, mean=[0.4588,0.4588,0.4588], std=[0.4588,0.4588,0.4588]):\n",
        "        '''\n",
        "        PORPOUSE: Perform the evaluation of an image and plot it\n",
        "\n",
        "        TAKES:\n",
        "          - image_tensor: tensor representing the image \n",
        "          - class_names: labels name of our image\n",
        "          - transform\n",
        "          - mean\n",
        "          - std\n",
        "\n",
        "        RETURNS: None\n",
        "        '''\n",
        "\n",
        "        topk = 3 # Number of top k labels that we want to see\n",
        "\n",
        "        if transform is not None: image_transform = transform\n",
        "        else: image_transform = transforms.Compose([ transforms.Normalize(mean=mean, std=std) ])\n",
        "        \n",
        "        self.model.to(self.device) # Move the model to device\n",
        "\n",
        "        self.model.eval() # Evauation phase\n",
        "        \n",
        "        with torch.inference_mode(): # Allow inference mode\n",
        "            transformed_image = image_transform(images_tensor).unsqueeze(dim=0)\n",
        "            targets_image_pred = self.model(transformed_image.to(self.device)) # Get the model output\n",
        "\n",
        "        target_image_pred_probs = torch.sigmoid(targets_image_pred).to('cpu')\n",
        "        top_k_probs_labels = torch.topk(target_image_pred_probs, k=topk, dim=1) # Get the probability of the top K labels\n",
        "\n",
        "        label_pred_names = [LABELS[int(lab.numpy())]for lab in top_k_probs_labels.indices[0]] # Get the name of the top K labels\n",
        "\n",
        "\n",
        "        # Images plot\n",
        "\n",
        "        print(f'True Labels:\\t{class_names}\\nTop {topk} Pred:\\t{label_pred_names}\\nTop {topk} Prob:\\t{top_k_probs_labels.values[0].numpy().tolist()}')\n",
        "\n",
        "\n",
        "        if(len(images_tensor.shape) == 3):\n",
        "            plt.figure(figsize=(5,5))\n",
        "            plt.title(f\"Model: {self.model_name}\")\n",
        "            plt.imshow(images_tensor.permute(1, 2, 0).numpy())\n",
        "            plt.axis(False)\n",
        "            plt.show()\n",
        "        else:\n",
        "            ids_frames = []\n",
        "            if images_tensor.shape[0] == 2: ids_frames = [1, -1]\n",
        "            elif images_tensor.shape[0] == 5: ids_frames = np.arange(5).tolist()\n",
        "            else: ids_frames = np.arange(10).tolist()\n",
        "\n",
        "            tb = widgets.TabBar([f'Frame {str(i)}' for i in ids_frames])\n",
        "\n",
        "            for img in range(0, images_tensor.shape[0]):\n",
        "                with tb.output_to(img, select=(img == 0)):\n",
        "                    plt.title(f\"Model: {self.model_name} - Frame: {ids_frames[img]}\")\n",
        "                    plt.imshow(images_tensor[img].permute(1, 2, 0).numpy())\n",
        "                    plt.axis(False)\n",
        "                    plt.show()\n",
        "                    \n",
        "                  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0Qg5iACv3MV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "PORPOUSE: Perform Accuracy Score\n",
        "\n",
        "TAKES:\n",
        "  - outputs: model output\n",
        "  - labels: ground truth\n",
        "\n",
        "RETURNS: accuracy score\n",
        "'''\n",
        "\n",
        "def accuracy_score(outputs, labels):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item()\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztfRyKzxF64O"
      },
      "source": [
        "# Let's run all!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOGOoENIwmIM"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 5\n",
        "n_classes = len(LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpeIH64PUI7r"
      },
      "outputs": [],
      "source": [
        "def plot_loss_curves(results_info):\n",
        "    '''\n",
        "    PORPOUSE: Plot Loss and Score curves\n",
        "\n",
        "    TAKES:\n",
        "      - results_info: dictionary containing the results and model name\n",
        "\n",
        "    RETURNS: None\n",
        "    '''\n",
        "\n",
        "    res = results_info['results']\n",
        "    epochs = range(len(res['train_loss']))\n",
        "\n",
        "    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (15, 7))\n",
        "\n",
        "    plt.title(results_info['model_name'])\n",
        "\n",
        "    ax[0].plot(epochs, res['train_loss'], label = 'train_loss')\n",
        "    ax[0].plot(epochs, res['val_loss'], label = 'val_loss')\n",
        "    ax[0].set_title('Loss - Epochs')\n",
        "    ax[0].set_xlabel('Epochs')\n",
        "    ax[0].set_ylabel('Loss')\n",
        "    ax[0].grid()\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(epochs, res['train_accuracy'], label = 'train_accuracy_score')\n",
        "    ax[1].plot(epochs, res['val_accuracy'], label = 'val_accuracy_score')\n",
        "    ax[1].set_title('Accuracy Score - Epochs')\n",
        "    ax[1].set_ylabel('Accuracy Score')\n",
        "    ax[1].set_xlabel('Epochs')\n",
        "    ax[1].grid()\n",
        "    ax[1].legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjnnZL4XUD1x"
      },
      "outputs": [],
      "source": [
        "def train_evaluate(model, test, epochs=NUM_EPOCHS):\n",
        "    '''\n",
        "    PORPOUSE: Perform the training and evaluation plotting the results\n",
        "\n",
        "    TAKES:\n",
        "      - model: CNN model to use\n",
        "      - test: test dataloader\n",
        "      - EPOCHS: numer of times that our model will see the data\n",
        "\n",
        "    RETURNS: Train history and results from test evaluation\n",
        "    '''\n",
        "\n",
        "    start_time = timer()\n",
        "    history = model.fit(NUM_EPOCHS)  # Train the model, it returns {'model_name': model_name, 'results': results}\n",
        "    end_time = timer()\n",
        "\n",
        "    print(f'Total training time: {end_time-start_time:.3f} seconds')\n",
        "\n",
        "    plot_loss_curves(history) # Compare the results between train and validation set\n",
        "\n",
        "    start_time = timer()\n",
        "    result = model.evaluate(test) # Evaluate the model in the Tran dataloader\n",
        "    # It returns {'model_name': model_name, 'model_loss': val_loss.item(), 'model_accuracy': val_accuracy.item()}\n",
        "    end_time = timer()\n",
        "\n",
        "\n",
        "    print(f'Total evaluation time: {end_time-start_time:.3f} seconds\\n')\n",
        "    print(f\"TEST Results for {result['model_name']} -> loss: {result['model_loss']} accuracy-score: {result['model_accuracy']}\")\n",
        "    \n",
        "    return (history, result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR3T8ZsbR7bW"
      },
      "outputs": [],
      "source": [
        "def test_single_image(dl, dic, cnn_arc):\n",
        "    '''\n",
        "    PORPOUSE: Perform the evaluation given a single image\n",
        "\n",
        "    TAKES:\n",
        "      - dl: image dataloader\n",
        "      - dic: image dictionary\n",
        "      - cnn_arc: CNN model\n",
        "\n",
        "    RETURNS: None \n",
        "    '''\n",
        "\n",
        "    rnd = random.choice(dl)\n",
        "    img, label, path = rnd\n",
        "    _, label_name = get_label_ids_names(dic, path)\n",
        "    cnn_arc.evaluate_and_plot_image(img, label_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LbXlsTRVnwV"
      },
      "outputs": [],
      "source": [
        "def calculate_pos_weights(train_df, test_df, n_classes):\n",
        "    '''\n",
        "    PORPOUSE: Calculate the class weights since we have an imbalance dataset\n",
        "\n",
        "    TAKES:\n",
        "      - train_ds: train dataframe\n",
        "      - test_ds: test dataframe\n",
        "      - n_classes: number of classes\n",
        "\n",
        "    RETURNS: np.array of weights\n",
        "    '''\n",
        "\n",
        "    col = list(map(str, range(n_classes)))\n",
        "\n",
        "    count_train = train_df[col].sum(axis=0).to_numpy()\n",
        "    count_test = test_df[col].sum(axis=0).to_numpy()\n",
        "    class_counts = count_train + count_test\n",
        "\n",
        "    l = len(train_df) + len(test_df)\n",
        "\n",
        "    pos_weights = np.ones_like(class_counts)\n",
        "    neg_counts = [l - pos_count for pos_count in class_counts]\n",
        "\n",
        "\n",
        "    for cdx, (pos_count, neg_count) in enumerate(zip(class_counts.tolist(),  neg_counts)):\n",
        "        pos_weights[cdx] = neg_count / (pos_count + 1e-5) # asserts also division by zero since we add 1e-5\n",
        "\n",
        "    return torch.as_tensor(pos_weights, dtype=torch.float).to(device)\n",
        "\n",
        "weights = calculate_pos_weights(train_df, test_df, n_classes)\n",
        "#weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBz1KYqz361D"
      },
      "source": [
        "Dataloaders definitons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLqx2oQoI6kM"
      },
      "outputs": [],
      "source": [
        "train_dl_single, val_dl_single, test_dl_single = generate_dataloaders(train_data_single, val_data_single, test_data_single)\n",
        "train_dl_late, val_dl_late, test_dl_late = generate_dataloaders(train_data_late, val_data_late, test_data_late)\n",
        "train_dl_early, val_dl_early, test_dl_early = generate_dataloaders(train_data_early, val_data_early, test_data_early)\n",
        "train_dl_slow, val_dl_slow, test_dl_slow = generate_dataloaders(train_data_slow, val_data_slow, test_data_slow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxxWz9kSpEDQ"
      },
      "source": [
        "Defining the container of the experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16IgOAfnpDkJ"
      },
      "outputs": [],
      "source": [
        "experiments_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHy96tKkQ2da"
      },
      "source": [
        "## Single Frame Model CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9mt0-8fQ6OR"
      },
      "outputs": [],
      "source": [
        "AN_single = AlexNet(in_channels=3) # Extractor\n",
        "Single_CNN = SingleResCNN(AN_single, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_single = torch.optim.SGD(params=Single_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_single = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_single, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_single = CNN_Architecture(model = Single_CNN, \n",
        "             train_dataloader = train_dl_single,\n",
        "             val_dataloader = val_dl_single,\n",
        "             optimizer = optimizer_single,\n",
        "             loss_fn = nn.CrossEntropyLoss(weight=weights),\n",
        "             score_fn = accuracy_score,\n",
        "             scheduler = scheduler_single,\n",
        "             device = device,\n",
        "             save_check = True,\n",
        "             load_check_train = False,\n",
        "             load_check_evaluate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UooZ50TaotSh"
      },
      "outputs": [],
      "source": [
        "experiments_results.append(train_evaluate(CNN_arch_single, test_dl_single)) # Run train and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk6xVUEnkZdI"
      },
      "outputs": [],
      "source": [
        "test_single_image(test_data_single, test_dict, CNN_arch_single) # Test in a single random image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65rlij38SdzA"
      },
      "source": [
        "## Single Frame Model Fovea CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asd7eEk9Sj9i"
      },
      "outputs": [],
      "source": [
        "AN_Fovea = AlexNet(in_channels=3, stream_type='fovea') # Extractor\n",
        "Fovea_CNN = SingleResCNN(AN_Fovea, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_fovea = torch.optim.SGD(params=Fovea_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_fovea = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_fovea, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_fovea = CNN_Architecture(model = Fovea_CNN, \n",
        "             train_dataloader = train_dl_single,\n",
        "             val_dataloader = val_dl_single,\n",
        "             optimizer = optimizer_fovea,\n",
        "             loss_fn = nn.CrossEntropyLoss(weight=weights),\n",
        "             score_fn = accuracy_score,\n",
        "             scheduler = scheduler_fovea,\n",
        "             device = device,\n",
        "             save_check = True,\n",
        "             load_check_train = True,\n",
        "             load_check_evaluate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6EK2UB6szaX"
      },
      "outputs": [],
      "source": [
        "experiments_results.append(train_evaluate(CNN_arch_fovea, test_dl_single)) # Run train and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlnRA81VsyJE"
      },
      "outputs": [],
      "source": [
        "test_single_image(test_data_single, test_dict, CNN_arch_fovea) # Test in a single random image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRbO3-84S-js"
      },
      "source": [
        "## Single Frame Model Context CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP00WwvQTD9N"
      },
      "outputs": [],
      "source": [
        "AN_Context = AlexNet(in_channels=3, stream_type='context') # Extractor\n",
        "Context_CNN = SingleResCNN(AN_Context, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_context = torch.optim.SGD(params=Context_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_context = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_context, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_context = CNN_Architecture(model = Context_CNN, \n",
        "             train_dataloader = train_dl_single,\n",
        "             val_dataloader = val_dl_single,\n",
        "             optimizer = optimizer_context,\n",
        "             loss_fn = nn.CrossEntropyLoss(weight=weights),\n",
        "             score_fn = accuracy_score,\n",
        "             scheduler = scheduler_context,\n",
        "             device = device,\n",
        "             save_check = True,\n",
        "             load_check_train = True,\n",
        "             load_check_evaluate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJIYUGJStEFz"
      },
      "outputs": [],
      "source": [
        "experiments_results.append(train_evaluate(CNN_arch_context, test_dl_single)) # Run train and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRuXA7uHtFwP"
      },
      "outputs": [],
      "source": [
        "test_single_image(test_data_single, test_dict, CNN_arch_context) # Test in a single random image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh7rm0-PTUIK"
      },
      "source": [
        "## Single Frame Model Multiresolution CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m__Ss3cLTTtt"
      },
      "outputs": [],
      "source": [
        "AN_Multi_Fovea, AN_Multi_context = AlexNet(in_channels=3, stream_type='fovea'), AlexNet(in_channels=3, stream_type='context') # Extractors\n",
        "Multi_CNN = MultiResCNN(AN_Multi_Fovea, AN_Multi_context, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_multi = torch.optim.SGD(params=Multi_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_multi = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_multi, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_multi = CNN_Architecture(model = Multi_CNN, \n",
        "             train_dataloader = train_dl_single,\n",
        "             val_dataloader = val_dl_single,\n",
        "             optimizer = optimizer_multi,\n",
        "             loss_fn = nn.CrossEntropyLoss(weight=weights),\n",
        "             score_fn = accuracy_score,\n",
        "             scheduler = scheduler_multi,\n",
        "             device = device,\n",
        "             save_check = True,\n",
        "             load_check_train = True,\n",
        "             load_check_evaluate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdRfMVrhtIbW"
      },
      "outputs": [],
      "source": [
        "experiments_results.append(train_evaluate(CNN_arch_multi, test_dl_single)) # Run train and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGfrZK99tJ6L"
      },
      "outputs": [],
      "source": [
        "test_single_image(test_data_single, test_dict, CNN_arch_multi) # Test in a single random image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gtKCo7fTidh"
      },
      "source": [
        "## Multi Frame Model Late Fusion CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZzRfYX3TrEN"
      },
      "outputs": [],
      "source": [
        "AN_Late1, AN_late2 = AlexNet(in_channels=3), AlexNet(in_channels=3) # Extractors\n",
        "Late_CNN = LateFusionCNN(AN_Late1, AN_late2, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_late = torch.optim.SGD(params=Late_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_late = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_late, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_late = CNN_Architecture(model = Late_CNN, \n",
        "             train_dataloader = train_dl_late,\n",
        "             val_dataloader = val_dl_late,\n",
        "             optimizer = optimizer_late,\n",
        "             loss_fn = nn.CrossEntropyLoss(weight=weights),\n",
        "             score_fn = accuracy_score,\n",
        "             scheduler = scheduler_late,\n",
        "             device = device,\n",
        "             save_check = True,\n",
        "             load_check_train = True,\n",
        "             load_check_evaluate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Dw4PGEztONY"
      },
      "outputs": [],
      "source": [
        "experiments_results.append(train_evaluate(CNN_arch_late, test_dl_late)) # Run train and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoZiZLprtL1e"
      },
      "outputs": [],
      "source": [
        "test_single_image(test_data_late, test_dict, CNN_arch_late) # Test in a single random image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqYxdZBbUfqU"
      },
      "source": [
        "## Multi Frame Model Early Fusion CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjP_CL0JUDLR"
      },
      "outputs": [],
      "source": [
        "AN_Early = AlexNet(in_channels=3, t_frames=[5,1,1]) # Extractor\n",
        "Early_CNN = EarlyFusionCNN(AN_Early, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_early = torch.optim.SGD(params=Early_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_early = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_early, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_early = CNN_Architecture(model = Early_CNN, \n",
        "             train_dataloader = train_dl_early,\n",
        "             val_dataloader = val_dl_early,\n",
        "             optimizer = optimizer_early,\n",
        "             loss_fn = nn.CrossEntropyLoss(weight=weights),\n",
        "             score_fn = accuracy_score,\n",
        "             scheduler = scheduler_early,\n",
        "             device = device,\n",
        "             save_check = True,\n",
        "             load_check_train = True,\n",
        "             load_check_evaluate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDB2D8BbtRxN"
      },
      "outputs": [],
      "source": [
        "experiments_results.append(train_evaluate(CNN_arch_early, test_dl_early)) # Run train and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nYUZ8PXtTiz"
      },
      "outputs": [],
      "source": [
        "test_single_image(test_data_early, test_dict, CNN_arch_early) # Test in a single random image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WGmsIQOUqPp"
      },
      "source": [
        "## Multi Frame Model Slow Fusion CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO8Qvod3Usw3"
      },
      "outputs": [],
      "source": [
        "AN_Slow = AlexNet(in_channels=3, t_frames=[4,2,2]) # Extractor\n",
        "Slow_CNN = SlowFusionCNN(AN_Slow, num_classes=n_classes) # CNN model\n",
        "\n",
        "optimizer_slow = torch.optim.SGD(params=Slow_CNN.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-5) # Optimizer \n",
        "scheduler_slow = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_slow, factor=0.1, patience=2, verbose=True) # Scheduler\n",
        "\n",
        "# Architecture definition\n",
        "CNN_arch_slow = CNN_Architecture(model = Slow_CNN, \n",
        "             train_dataloader = train_dl_slow,\n",
        "             val_dataloader = val_dl_slow,\n",
        "             optimizer = optimizer_slow,\n",
        "             loss_fn = nn.CrossEntropyLoss(weight=weights),\n",
        "             score_fn = accuracy_score,\n",
        "             scheduler = scheduler_slow,\n",
        "             device = device,\n",
        "             save_check = True,\n",
        "             load_check_train = True,\n",
        "             load_check_evaluate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXPaEJXltVEG"
      },
      "outputs": [],
      "source": [
        "experiments_results.append(train_evaluate(CNN_arch_slow, test_dl_slow)) # Run train and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndK5ZImttXJm"
      },
      "outputs": [],
      "source": [
        "test_single_image(test_data_slow, test_dict, CNN_arch_slow) # Test in a single random image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VDxTtATTu8z"
      },
      "source": [
        "## Result Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d0PrnAjqzc6"
      },
      "source": [
        "Summary Widget with all the previous loss & score plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOIwzwcXqqSf"
      },
      "outputs": [],
      "source": [
        "if (len(experiments_results) > 0):\n",
        "    tb = widgets.TabBar([exp[0]['model_name'] for exp in experiments_results])\n",
        "\n",
        "    for idx, exp in enumerate(experiments_results):\n",
        "        history, _ = exp\n",
        "        with tb.output_to(history['model_name'], select=(idx == 0)):\n",
        "            plot_loss_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzuf7A5Bq7_T"
      },
      "source": [
        "Summary of test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rtRuy0hrDML"
      },
      "outputs": [],
      "source": [
        "if (len(experiments_results) > 0):\n",
        "    test_results_grid = widgets.Grid(len(experiments_results) + 1, 3, header_row=True)\n",
        "\n",
        "    with test_results_grid.output_to(0, 0): print('Model Name')\n",
        "    with test_results_grid.output_to(0, 1): print('Loss')\n",
        "    with test_results_grid.output_to(0, 2): print('Accuracy Score')\n",
        "\n",
        "    for row, exp in enumerate(experiments_results):\n",
        "        _, results = exp\n",
        "        for col, val in enumerate(results.values()):\n",
        "            with test_results_grid.output_to(row + 1, col): print(val)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uRVs6r74wQDu",
        "aZGMVBhfbwb9"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f58b3eb352994a65a2bbf0bce2288b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce4d2a7fccf04446af789387bc4b4c9e",
              "IPY_MODEL_e68efad9e7e245feae3b810d59efc489",
              "IPY_MODEL_b22b3c1ef1974219adbac12f6ac18b3d"
            ],
            "layout": "IPY_MODEL_c3bbc4dd900b483b81078d55b2b48c07"
          }
        },
        "ce4d2a7fccf04446af789387bc4b4c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12c2d3c68c63440e96cab644622c8ac6",
            "placeholder": "​",
            "style": "IPY_MODEL_7a96039ea52944479933d5b3906299f6",
            "value": "Unzipping train.zip:   2%"
          }
        },
        "e68efad9e7e245feae3b810d59efc489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbc201a49d38481dae6f5b86372bead3",
            "max": 1592774,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a78dda45698841a28bcb4babd9b522df",
            "value": 32325
          }
        },
        "b22b3c1ef1974219adbac12f6ac18b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2fbc3fe3e19452588443c626bd5753a",
            "placeholder": "​",
            "style": "IPY_MODEL_2e2a9235ea0a467fbd26b827363cef1f",
            "value": " 32325/1592774 [00:27&lt;59:40, 435.81it/s]"
          }
        },
        "c3bbc4dd900b483b81078d55b2b48c07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12c2d3c68c63440e96cab644622c8ac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a96039ea52944479933d5b3906299f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbc201a49d38481dae6f5b86372bead3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78dda45698841a28bcb4babd9b522df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2fbc3fe3e19452588443c626bd5753a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e2a9235ea0a467fbd26b827363cef1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}